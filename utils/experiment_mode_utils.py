'''å®éªŒæ¨¡å¼å·¥å…·,ä¸æ•°å­¦æ¨¡å¼ä¸å®Œå…¨ç›¸åŒï¼Œä¸»è¦æ˜¯ç»˜åˆ¶å°½å¯èƒ½å¤šçš„ç©¿è¿‡æ•°æ®ç‚¹çš„å…‰æ»‘æ›²çº¿
éœ€è¦è¿‡æ»¤è¶…å‡ºè®¾å®šé˜ˆå€¼çš„æ•°æ®å¼‚å¸¸ç‚¹
å¦‚æœé˜ˆå€¼ä¸º0ï¼Œåˆ™ä¸è¿›è¡Œè¿‡æ»¤
å®éªŒæ¨¡å¼ä¼šå¯¹è¾“å…¥çš„æ•°æ®è¿›è¡Œåˆ†æï¼Œä¸»è¦æ˜¯å¯¹ç»˜åˆ¶å‡ºçš„æ›²çº¿è¿›è¡Œåˆç†æ€§åˆ†æï¼Œåˆ¤æ–­
æ ¹æ®å·²çŸ¥çš„æ•°æ®ç‚¹åˆ¤æ–­åœ¨åˆç†çš„è¯¯å·®å…è®¸èŒƒå›´å†…æ›²çº¿æ˜¯å¦å…·æœ‰ä»£è¡¨æ€§
æ˜¾ç¤ºå®éªŒæ•°æ®å¤„ç†æ—¶çš„ç»Ÿè®¡å­¦æ•°æ®ï¼Œç”¨äºè¾…åŠ©è®¡ç®—ä¸ç¡®å®šåº¦å’Œè¯¯å·®å€¼
'''

import numpy as np
from scipy import interpolate
from typing import Tuple, List, Dict, Any
from .fitting_functions import filter_outliers, calculate_statistics, generate_curve_points, perform_fitting

def experiment_mode_analysis(x_data: np.ndarray, y_data: np.ndarray, enable_outlier_filter: bool = False, 
                           outlier_threshold: float = 3.0, fit_method: str = 'å¤šé¡¹å¼æ‹Ÿåˆ', 
                           enable_iterative_filter: bool = False, iteration_count: int = 3, 
                           iteration_threshold: float = 0.1) -> Dict[str, Any]:
    """å®éªŒæ¨¡å¼çš„æ•°æ®å¤„ç†å’Œåˆ†æä¸»å‡½æ•°ï¼Œå¢å¼ºç‰ˆï¼Œæ”¯æŒå¤šç§æ‹Ÿåˆæ–¹æ³•"""
    results = {
        'original_data': (x_data.copy(), y_data.copy()),
        'filtered_data': (x_data.copy(), y_data.copy()),  # é»˜è®¤ä½¿ç”¨åŸå§‹æ•°æ®
        'filtered_indices': [],
        'threshold_used': 0.0,
        'filtered_stats': None,
        'best_poly_fit': None,
        'smooth_curve': None,
        'curve_quality': None
    }
    
    # æ•°æ®éªŒè¯
    if len(x_data) != len(y_data) or len(x_data) < 2:
        results['curve_quality'] = {
            'goodness_of_fit': 'æœªçŸ¥',
            'error_analysis': {},
            'data_representativeness': 'æœªçŸ¥',
            'recommendations': ['æ•°æ®ç‚¹æ•°é‡ä¸è¶³æˆ–X-Yæ•°æ®é•¿åº¦ä¸åŒ¹é…'],
            'uncertainty_estimates': {},
            'residual_analysis': {},
            'confidence_measures': {}
        }
        return results
    
    # è¿‡æ»¤å¼‚å¸¸ç‚¹ï¼ˆå¦‚æœéœ€è¦ï¼‰
    if enable_outlier_filter and outlier_threshold > 0:
        filtered_x, filtered_y, filtered_indices = filter_outliers(x_data, y_data, threshold=outlier_threshold)
        results['filtered_data'] = (filtered_x, filtered_y)
        results['filtered_indices'] = filtered_indices
        results['threshold_used'] = outlier_threshold
    else:
        # ä¸è¿›è¡Œè¿‡æ»¤ï¼Œä½¿ç”¨åŸå§‹æ•°æ®
        results['filtered_data'] = (x_data.copy(), y_data.copy())
        results['filtered_indices'] = []
        results['threshold_used'] = 0.0
    
    # æ‰§è¡Œè¿­ä»£è¿‡æ»¤
    if enable_iterative_filter and iteration_count > 0 and iteration_threshold > 0:
        # è®°å½•è¿­ä»£è¿‡æ»¤çš„å†å²
        iteration_history = []
        current_x, current_y = results['filtered_data']
        current_indices = np.arange(len(current_x))
        original_total_indices = len(x_data)
        
        for iteration in range(iteration_count):
            # æ‰§è¡Œæ‹Ÿåˆä»¥è·å–æ›²çº¿
            temp_results = {}
            if fit_method == 'å¤šé¡¹å¼æ‹Ÿåˆ':
                # æ‰¾åˆ°æœ€ä½³å¤šé¡¹å¼æ‹Ÿåˆ
                poly_fit = find_best_polynomial_fit(current_x, current_y)
                temp_results['best_poly_fit'] = poly_fit
                
                if poly_fit:
                    # è®¡ç®—å½“å‰æ•°æ®ç‚¹åœ¨æ‹Ÿåˆæ›²çº¿ä¸Šçš„é¢„æµ‹å€¼
                    y_pred = np.polyval(poly_fit['coeffs'], current_x)
                else:
                    break  # å¦‚æœæ‹Ÿåˆå¤±è´¥ï¼Œåœæ­¢è¿­ä»£
            else:  # å¹³æ»‘æ ·æ¡æ‹Ÿåˆ
                try:
                    # å¯¹xæ•°æ®è¿›è¡Œæ’åºä»¥ç¡®ä¿æ ·æ¡æ’å€¼çš„æ­£ç¡®æ€§
                    sorted_indices = np.argsort(current_x)
                    sorted_x = current_x[sorted_indices]
                    sorted_y = current_y[sorted_indices]
                    
                    # ä½¿ç”¨åˆé€‚çš„æ’å€¼æ–¹æ³•
                    if len(sorted_x) >= 4:
                        spl = interpolate.CubicSpline(sorted_x, sorted_y)
                    else:
                        spl = interpolate.interp1d(sorted_x, sorted_y, kind='linear')
                    
                    # è®¡ç®—å½“å‰æ•°æ®ç‚¹åœ¨æ‹Ÿåˆæ›²çº¿ä¸Šçš„é¢„æµ‹å€¼
                    y_pred = spl(current_x)
                except Exception:
                    break  # å¦‚æœæ‹Ÿåˆå¤±è´¥ï¼Œåœæ­¢è¿­ä»£
            
            # è®¡ç®—æ®‹å·®
            residuals = np.abs(current_y - y_pred)
            
            # è®¡ç®—ç›¸å¯¹è¯¯å·®é˜ˆå€¼ï¼ˆå¦‚æœæŒ‡å®šäº†ç›¸å¯¹è¯¯å·®ï¼‰
            if iteration_threshold < 1.0:  # å‡è®¾å°äº1.0è¡¨ç¤ºç›¸å¯¹è¯¯å·®
                # ä½¿ç”¨å½“å‰yå€¼çš„èŒƒå›´æˆ–å‡å€¼ä½œä¸ºåŸºå‡†
                y_range = np.max(current_y) - np.min(current_y)
                if y_range > 0:
                    absolute_threshold = iteration_threshold * y_range
                else:
                    absolute_threshold = iteration_threshold * np.mean(current_y) if np.mean(current_y) > 0 else iteration_threshold
            else:
                absolute_threshold = iteration_threshold  # ç»å¯¹å€¼é˜ˆå€¼
            
            # è¿‡æ»¤è¶…å‡ºé˜ˆå€¼çš„ç‚¹
            mask = residuals <= absolute_threshold
            
            # å¦‚æœæ²¡æœ‰è¿‡æ»¤æ‰ä»»ä½•ç‚¹ï¼Œåœæ­¢è¿­ä»£
            if np.all(mask):
                break
            
            # æ›´æ–°æ•°æ®å’Œç´¢å¼•
            iteration_history.append({
                'iteration': iteration + 1,
                'removed_count': len(current_x) - np.sum(mask),
                'remaining_count': np.sum(mask),
                'threshold_used': absolute_threshold
            })
            
            # æ›´æ–°å½“å‰æ•°æ®
            current_x = current_x[mask]
            current_y = current_y[mask]
            current_indices = current_indices[mask]
            
            # å¦‚æœæ•°æ®ç‚¹å°‘äº5ä¸ªï¼Œåœæ­¢è¿­ä»£
            if len(current_x) < 5:
                break
        
        # ç¡®å®šåŸå§‹æ•°æ®ä¸­çš„ç´¢å¼•ï¼ˆå¦‚æœéœ€è¦ï¼‰
        final_filtered_indices = []
        if len(iteration_history) > 0:
            # è®¡ç®—è¢«è¿‡æ»¤æ‰çš„ç´¢å¼•
            all_indices = np.arange(original_total_indices)
            if results['filtered_indices']:  # å¦‚æœä¹‹å‰æœ‰è¿‡æ»¤
                # è·å–æœªè¢«åˆå§‹è¿‡æ»¤çš„ç´¢å¼•
                initial_kept_indices = [i for i in all_indices if i not in results['filtered_indices']]
                # åœ¨åˆå§‹ä¿ç•™çš„ç´¢å¼•ä¸­æ‰¾åˆ°è¢«è¿­ä»£è¿‡æ»¤æ‰çš„
                kept_in_iteration = [initial_kept_indices[i] for i in current_indices]
                # æ‰€æœ‰è¢«è¿‡æ»¤çš„ç´¢å¼• = åˆå§‹è¿‡æ»¤çš„ + è¿­ä»£è¿‡æ»¤çš„
                final_filtered_indices = results['filtered_indices'] + [i for i in initial_kept_indices if i not in kept_in_iteration]
            else:
                # ç›´æ¥è®¡ç®—è¢«è¿­ä»£è¿‡æ»¤æ‰çš„ç´¢å¼•
                kept_in_iteration = [all_indices[i] for i in current_indices]
                final_filtered_indices = [i for i in all_indices if i not in kept_in_iteration]
            
            # æ›´æ–°ç»“æœ
            results['filtered_data'] = (current_x, current_y)
            results['filtered_indices'] = final_filtered_indices
            results['iteration_history'] = iteration_history
    
    # è·å–è¿‡æ»¤åçš„æ•°æ®
    filtered_x, filtered_y = results['filtered_data']
    
    # å¦‚æœè¿‡æ»¤åæ•°æ®ç‚¹å¤ªå°‘ï¼Œè¿”å›é”™è¯¯ç»“æœ
    if len(filtered_x) < 2:
        results['curve_quality'] = {
            'goodness_of_fit': 'æœªçŸ¥',
            'error_analysis': {},
            'data_representativeness': 'æœªçŸ¥',
            'recommendations': ['è¿‡æ»¤åæ•°æ®ç‚¹å¤ªå°‘ï¼Œæ— æ³•è¿›è¡Œæœ‰æ•ˆæ‹Ÿåˆ'],
            'uncertainty_estimates': {},
            'residual_analysis': {},
            'confidence_measures': {}
        }
        return results
    
    # è®¡ç®—è¿‡æ»¤åæ•°æ®çš„å¢å¼ºç»Ÿè®¡ä¿¡æ¯
    # åŸºç¡€ç»Ÿè®¡é‡
    mean_x = np.mean(filtered_x)
    std_x = np.std(filtered_x)
    mean_y = np.mean(filtered_y)
    std_y = np.std(filtered_y)
    
    # è®¡ç®—ä¸­ä½æ•°
    median_x = np.median(filtered_x)
    median_y = np.median(filtered_y)
    
    # è®¡ç®—å››åˆ†ä½æ•°
    q1_x = np.percentile(filtered_x, 25)
    q3_x = np.percentile(filtered_x, 75)
    q1_y = np.percentile(filtered_y, 25)
    q3_y = np.percentile(filtered_y, 75)
    
    # è®¡ç®—å››åˆ†ä½è·
    iqr_x = q3_x - q1_x
    iqr_y = q3_y - q1_y
    
    # è®¡ç®—å˜å¼‚ç³»æ•°
    cv_x = (std_x / mean_x * 100) if mean_x != 0 else 0
    cv_y = (std_y / mean_y * 100) if mean_y != 0 else 0
    
    # è®¡ç®—ç›¸å…³ç³»æ•°
    correlation = np.corrcoef(filtered_x, filtered_y)[0, 1] if len(filtered_x) > 1 else 0
    
    # ç»„åˆå¢å¼ºçš„ç»Ÿè®¡ä¿¡æ¯
    results['filtered_stats'] = {
        'n_points': len(filtered_x),
        'min_x': np.min(filtered_x),
        'max_x': np.max(filtered_x),
        'mean_x': mean_x,
        'median_x': median_x,
        'std_x': std_x,
        'q1_x': q1_x,
        'q3_x': q3_x,
        'iqr_x': iqr_x,
        'cv_x': cv_x,
        'min_y': np.min(filtered_y),
        'max_y': np.max(filtered_y),
        'mean_y': mean_y,
        'median_y': median_y,
        'std_y': std_y,
        'q1_y': q1_y,
        'q3_y': q3_y,
        'iqr_y': iqr_y,
        'cv_y': cv_y,
        'correlation': correlation
    }
    
    # æ ¹æ®é€‰æ‹©çš„æ‹Ÿåˆæ–¹æ³•æ‰§è¡Œæ‹Ÿåˆ
    if fit_method == 'å¤šé¡¹å¼æ‹Ÿåˆ':
        # æ‰¾åˆ°æœ€ä½³å¤šé¡¹å¼æ‹Ÿåˆ
        results['best_poly_fit'] = find_best_polynomial_fit(filtered_x, filtered_y)
        
        # è¯„ä¼°æ‹Ÿåˆè´¨é‡
        if results['best_poly_fit']:
            results['curve_quality'] = evaluate_curve_quality(filtered_x, filtered_y, results['best_poly_fit'])
        else:
            # å¦‚æœå¤šé¡¹å¼æ‹Ÿåˆå¤±è´¥ï¼Œè®¾ç½®é»˜è®¤è´¨é‡
            results['curve_quality'] = {
                'goodness_of_fit': 'è¾ƒå·®',
                'error_analysis': {},
                'data_representativeness': 'ä¸€èˆ¬',
                'recommendations': ['å¤šé¡¹å¼æ‹Ÿåˆå¤±è´¥ï¼Œæ•°æ®å¯èƒ½ä¸é€‚åˆå¤šé¡¹å¼æ¨¡å‹'],
                'uncertainty_estimates': {},
                'residual_analysis': {},
                'confidence_measures': {}
            }
    else:  # å¹³æ»‘æ ·æ¡æ‹Ÿåˆ
        try:
            # å¯¹xæ•°æ®è¿›è¡Œæ’åºä»¥ç¡®ä¿æ ·æ¡æ’å€¼çš„æ­£ç¡®æ€§
            sorted_indices = np.argsort(filtered_x)
            sorted_x = filtered_x[sorted_indices]
            sorted_y = filtered_y[sorted_indices]
            
            # ä½¿ç”¨ä¸åŒçš„æ’å€¼æ–¹æ³•ï¼Œæ ¹æ®æ•°æ®é‡é€‰æ‹©åˆé€‚çš„æ–¹æ³•
            if len(sorted_x) >= 4:
                # å¯¹äºè¶³å¤Ÿçš„æ•°æ®ç‚¹ï¼Œä½¿ç”¨ä¸‰æ¬¡æ ·æ¡æ’å€¼
                spl = interpolate.CubicSpline(sorted_x, sorted_y)
            else:
                # å¯¹äºè¾ƒå°‘çš„æ•°æ®ç‚¹ï¼Œä½¿ç”¨çº¿æ€§æ’å€¼
                spl = interpolate.interp1d(sorted_x, sorted_y, kind='linear')
            
            # ç”Ÿæˆå¹³æ»‘æ›²çº¿çš„æ•°æ®ç‚¹
            x_min, x_max = min(sorted_x), max(sorted_x)
            # æ‰©å±•èŒƒå›´ä»¥æ›´å¥½åœ°æ˜¾ç¤ºæ›²çº¿
            x_min_extended = x_min - 0.1 * (x_max - x_min)
            x_max_extended = x_max + 0.1 * (x_max - x_min)
            x_smooth = np.linspace(x_min_extended, x_max_extended, 1000)
            
            # è®¡ç®—æ’å€¼åçš„yå€¼
            y_smooth = spl(x_smooth)
            
            # å­˜å‚¨å¹³æ»‘æ›²çº¿
            results['smooth_curve'] = (x_smooth, y_smooth)
            
            # è®¡ç®—å¹³æ»‘æ›²çº¿çš„æ‹Ÿåˆè´¨é‡
            # ç”±äºæ ·æ¡æ›²çº¿æ˜¯é€šè¿‡æ‰€æœ‰æ•°æ®ç‚¹çš„ï¼Œè®¡ç®—æ’å€¼æ—¶çš„è¯¯å·®
            residuals = []
            # è®¡ç®—åŸå§‹æ•°æ®ç‚¹ä¸Šçš„æ‹Ÿåˆè´¨é‡
            for x, y in zip(filtered_x, filtered_y):
                # æ‰¾åˆ°xåœ¨æ’åºåçš„æ•°ç»„ä¸­çš„ä½ç½®ï¼Œè®¡ç®—å¯¹åº”çš„yå€¼
                if x >= sorted_x[0] and x <= sorted_x[-1]:
                    y_fit = spl(x)
                    residuals.append(y - y_fit)
            
            residuals = np.array(residuals)
            mse = np.mean(residuals**2) if len(residuals) > 0 else 0
            rmse = np.sqrt(mse)
            r_squared = 1.0  # æ ·æ¡æ’å€¼åœ¨æ•°æ®ç‚¹ä¸Šåº”è¯¥å®Œå…¨åŒ¹é…
            
            # åˆ›å»ºä¸€ä¸ªæ¨¡æ‹Ÿçš„æœ€ä½³æ‹Ÿåˆç»“æœç”¨äºè´¨é‡è¯„ä¼°
            mock_best_fit = {
                'degree': 'æ ·æ¡',
                'coeffs': [],
                'mse': mse,
                'rmse': rmse,
                'r_squared': r_squared
            }
            
            # è¯„ä¼°æ‹Ÿåˆè´¨é‡
            results['curve_quality'] = evaluate_curve_quality(filtered_x, filtered_y, mock_best_fit)
        except Exception as e:
            # å¦‚æœæ ·æ¡æ‹Ÿåˆå¤±è´¥ï¼Œè®¾ç½®é»˜è®¤è´¨é‡
            results['curve_quality'] = {
                'goodness_of_fit': 'è¾ƒå·®',
                'error_analysis': {},
                'data_representativeness': 'ä¸€èˆ¬',
                'recommendations': [f'å¹³æ»‘æ ·æ¡æ‹Ÿåˆå¤±è´¥: {str(e)}'],
                'uncertainty_estimates': {},
                'residual_analysis': {},
                'confidence_measures': {}
            }
    
    # ç¡®ä¿curve_qualityå§‹ç»ˆå­˜åœ¨
    if not results['curve_quality']:
        results['curve_quality'] = {
            'goodness_of_fit': 'æœªçŸ¥',
            'error_analysis': {},
            'data_representativeness': 'æœªçŸ¥',
            'recommendations': ['æ‹Ÿåˆè¿‡ç¨‹ä¸­å‡ºç°æœªçŸ¥é”™è¯¯'],
            'uncertainty_estimates': {},
            'residual_analysis': {},
            'confidence_measures': {}
        }
    
    return results

def find_best_polynomial_fit(x_data: np.ndarray, y_data: np.ndarray, max_degree: int = 7) -> Dict[str, Any]:
    """åŸºäºAICå‡†åˆ™é€‰æ‹©æœ€ä¼˜å¤šé¡¹å¼é˜¶æ•°ï¼Œå¢å¼ºç‰ˆ"""
    best_fit = None
    best_aic = float('inf')
    all_fits = []  # å­˜å‚¨æ‰€æœ‰å°è¯•çš„æ‹Ÿåˆç»“æœï¼Œç”¨äºæ¯”è¾ƒ
    
    # è®¡ç®—ä¸åŒé˜¶æ•°çš„å¤šé¡¹å¼æ‹Ÿåˆ
    for degree in range(1, max_degree + 1):
        try:
            # æ‹Ÿåˆå¤šé¡¹å¼ï¼Œä½¿ç”¨æ›´å¤šå‚æ•°ä»¥æé«˜æ•°å€¼ç¨³å®šæ€§
            coeffs = np.polyfit(x_data, y_data, degree, full=False)
            
            # è®¡ç®—æ‹Ÿåˆå€¼å’Œæ®‹å·®
            poly_func = np.poly1d(coeffs)
            y_fit = poly_func(x_data)
            residuals = y_data - y_fit
            
            # è®¡ç®—è¯¯å·®æŒ‡æ ‡
            n = len(x_data)
            k = degree + 1  # å‚æ•°æ•°é‡
            mse = np.mean(residuals**2)
            rmse = np.sqrt(mse)
            mae = np.mean(np.abs(residuals))
            
            # è®¡ç®—RÂ²
            ss_total = np.sum((y_data - np.mean(y_data))**2)
            ss_residual = np.sum(residuals**2)
            r_squared = 1 - (ss_residual / ss_total) if ss_total > 0 else 0
            
            # è®¡ç®—è°ƒæ•´åçš„RÂ²
            adj_r_squared = 1 - ((1 - r_squared) * (n - 1) / (n - k - 1)) if n > k + 1 else 0
            
            # è®¡ç®—AIC
            aic = n * np.log(mse) + 2 * k
            
            # è®¡ç®—BIC (è´å¶æ–¯ä¿¡æ¯å‡†åˆ™)
            bic = n * np.log(mse) + k * np.log(n)
            
            # å­˜å‚¨å½“å‰é˜¶æ•°çš„æ‹Ÿåˆç»“æœ
            fit_result = {
                'degree': degree,
                'coeffs': coeffs,
                'mse': mse,
                'rmse': rmse,
                'mae': mae,
                'r_squared': r_squared,
                'adjusted_r_squared': adj_r_squared,
                'aic': aic,
                'bic': bic
            }
            
            all_fits.append(fit_result)
            
            # å¦‚æœAICæ›´å°ï¼Œåˆ™æ›´æ–°æœ€ä½³æ‹Ÿåˆ
            if aic < best_aic:
                best_aic = aic
                best_fit = fit_result
                
        except np.linalg.LinAlgError:
            # å¦‚æœæ‹Ÿåˆå¤±è´¥ï¼ˆå¯èƒ½ç”±äºæ•°å€¼é—®é¢˜ï¼‰ï¼Œè·³è¿‡è¯¥é˜¶æ•°
            continue
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆé€‚çš„æ‹Ÿåˆï¼Œè¿”å›ä¸€ä¸ªç®€å•çš„çº¿æ€§æ‹Ÿåˆ
    if best_fit is None and len(x_data) >= 2:
        try:
            coeffs = np.polyfit(x_data, y_data, 1)
            poly_func = np.poly1d(coeffs)
            y_fit = poly_func(x_data)
            residuals = y_data - y_fit
            n = len(x_data)
            mse = np.mean(residuals**2)
            rmse = np.sqrt(mse)
            ss_total = np.sum((y_data - np.mean(y_data))**2)
            ss_residual = np.sum(residuals**2)
            r_squared = 1 - (ss_residual / ss_total) if ss_total > 0 else 0
            
            best_fit = {
                'degree': 1,
                'coeffs': coeffs,
                'mse': mse,
                'rmse': rmse,
                'mae': np.mean(np.abs(residuals)),
                'r_squared': r_squared,
                'adjusted_r_squared': 1 - ((1 - r_squared) * (n - 1) / (n - 2)) if n > 2 else 0,
                'aic': n * np.log(mse) + 2,
                'bic': n * np.log(mse) + np.log(n)
            }
        except:
            # æœ€åçš„å¤‡é€‰æ–¹æ¡ˆ
            best_fit = {
                'degree': 1,
                'coeffs': np.array([0, np.mean(y_data)]),  # æ°´å¹³çº¿
                'mse': np.var(y_data),
                'rmse': np.std(y_data),
                'mae': np.mean(np.abs(y_data - np.mean(y_data))),
                'r_squared': 0,
                'adjusted_r_squared': 0,
                'aic': float('inf'),
                'bic': float('inf')
            }
    
    # å¦‚æœæœ‰å¤šä¸ªæ‹Ÿåˆç»“æœï¼Œè€ƒè™‘è¿‡æ‹Ÿåˆé£é™©
    if len(all_fits) > 1 and best_fit:
        # æ£€æŸ¥æœ€ä½³æ‹Ÿåˆæ˜¯å¦ä¸ºæœ€é«˜é˜¶æ•°ï¼Œå¦‚æœæ˜¯ï¼Œè€ƒè™‘æ˜¯å¦çœŸçš„å¿…è¦
        if best_fit['degree'] == max_degree:
            # æ¯”è¾ƒä¸æ¬¡é«˜é˜¶çš„æ€§èƒ½å·®å¼‚
            for fit in all_fits:
                if fit['degree'] == max_degree - 1:
                    # å¦‚æœæ¬¡é«˜é˜¶çš„AICä¸æœ€ä½³æ‹Ÿåˆç›¸å·®ä¸å¤§ï¼Œä½†é˜¶æ•°æ›´ä½
                    if (best_fit['aic'] - fit['aic']) < 2:  # AICå·®å€¼å°äº2ï¼Œè®¤ä¸ºæ²¡æœ‰æ˜¾è‘—å·®å¼‚
                        # åŒæ—¶æ£€æŸ¥RÂ²çš„å·®å¼‚
                        if (best_fit['r_squared'] - fit['r_squared']) < 0.05:  # RÂ²æå‡ä¸æ˜æ˜¾
                            # é€‰æ‹©è¾ƒä½é˜¶æ•°çš„æ‹Ÿåˆä»¥é¿å…è¿‡æ‹Ÿåˆ
                            best_fit = fit
                        break
        
        # æ‰¾å‡ºAICå€¼ä¸æœ€ä½³æ‹Ÿåˆç›¸å·®å°äº2çš„æ‰€æœ‰æ¨¡å‹
        similar_fits = [fit for fit in all_fits if abs(fit['aic'] - best_fit['aic']) < 2]
        
        # å¦‚æœæœ‰å¤šä¸ªAICç›¸è¿‘çš„æ¨¡å‹ï¼Œé€‰æ‹©é˜¶æ•°æœ€ä½çš„
        if len(similar_fits) > 1:
            # æŒ‰é˜¶æ•°æ’åº
            similar_fits.sort(key=lambda x: x['degree'])
            # é€‰æ‹©é˜¶æ•°æœ€ä½çš„æ¨¡å‹
            best_fit = similar_fits[0]
    
    return best_fit

def evaluate_curve_quality(x_data: np.ndarray, y_data: np.ndarray, poly_fit: Dict[str, Any]) -> Dict[str, Any]:
    """è¯„ä¼°æ‹Ÿåˆæ›²çº¿çš„è´¨é‡å’Œåˆç†æ€§ï¼Œå¢å¼ºç‰ˆï¼ŒåŸºäºæ®‹å·®æ­£æ€åˆ†å¸ƒåˆ†æ"""
    quality = {
        'goodness_of_fit': 'æœªçŸ¥',
        'error_analysis': {},
        'data_representativeness': 'æœªçŸ¥',
        'recommendations': [],
        'uncertainty_estimates': {},  # æ–°å¢ï¼šä¸ç¡®å®šåº¦ä¼°è®¡
        'residual_analysis': {},      # æ–°å¢ï¼šæ®‹å·®åˆ†æ
        'confidence_measures': {},     # æ–°å¢ï¼šç½®ä¿¡åº¦æŒ‡æ ‡
        'normality_analysis': {}       # æ–°å¢ï¼šæ­£æ€åˆ†å¸ƒåˆ†æ
    }
    
    if poly_fit is None or len(x_data) < 2:
        quality['recommendations'].append('æ•°æ®ç‚¹ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œæœ‰æ•ˆè¯„ä¼°')
        return quality
    
    # è®¡ç®—æ‹Ÿåˆå€¼å’Œæ®‹å·®ï¼ˆdelta = y_data - y0ï¼Œå…¶ä¸­y0æ˜¯æ‹Ÿåˆæ›²çº¿ä¸Šçš„yå€¼ï¼‰
    poly_func = np.poly1d(poly_fit['coeffs'])
    y0 = poly_func(x_data)  # ç”±è¾“å…¥çš„xå€¼è®¡ç®—å¾—æ›²çº¿ä¸Šçš„yå€¼
    delta = y_data - y0     # è¾“å…¥çš„yå€¼ä¸y0çš„å·®å€¼
    
    # è®¡ç®—è¯¯å·®ç»Ÿè®¡
    abs_delta = np.abs(delta)
    mean_y = np.mean(y_data)
    
    # åŸºç¡€è¯¯å·®åˆ†æ
    error_analysis = {
        'mean_absolute_error': np.mean(abs_delta),
        'max_absolute_error': np.max(abs_delta),
        'std_error': np.std(delta),
        'percent_points_within_1std': np.mean(abs_delta <= np.std(delta)) * 100,
        'percent_points_within_2std': np.mean(abs_delta <= 2 * np.std(delta)) * 100
    }
    
    # æ–°å¢ï¼šç›¸å¯¹è¯¯å·®è®¡ç®—
    if np.mean(np.abs(y_data)) > 0:
        error_analysis['mean_relative_error'] = np.mean(abs_delta / np.abs(y_data)) * 100
        error_analysis['max_relative_error'] = np.max(abs_delta / np.abs(y_data)) * 100
    
    quality['error_analysis'] = error_analysis
    
    # æ–°å¢ï¼šæ®‹å·®åˆ†æï¼ˆåŸºäºdeltaï¼‰
    residual_analysis = {
        'residual_mean': np.mean(delta),
        'residual_std': np.std(delta),
        'residual_skewness': np.mean(delta**3) / (np.std(delta)**3) if np.std(delta) > 0 else 0,
        'residual_kurtosis': np.mean(delta**4) / (np.std(delta)**4) - 3 if np.std(delta) > 0 else 0,
        'residual_min': np.min(delta),
        'residual_max': np.max(delta),
        'residual_range': np.max(delta) - np.min(delta),
        'abs_residual_mean': np.mean(abs_delta)  # å¹³å‡ç»å¯¹æ®‹å·®
    }
    
    # æ®‹å·®åˆ†å¸ƒç‰¹å¾ä¸è§£é‡Š
    if abs(residual_analysis['residual_skewness']) < 0.5:
        residual_analysis['distribution_shape'] = 'è¿‘ä¼¼å¯¹ç§°'
        residual_analysis['shape_interpretation'] = 'æ®‹å·®åˆ†å¸ƒæ¥è¿‘æ­£æ€åˆ†å¸ƒï¼Œæ‹Ÿåˆæ¨¡å‹å‡è®¾è¾ƒä¸ºåˆç†'
    elif residual_analysis['residual_skewness'] > 0:
        residual_analysis['distribution_shape'] = 'å³å'
        residual_analysis['shape_interpretation'] = 'å­˜åœ¨è¾ƒå¤šæ­£æ®‹å·®ï¼Œæ¨¡å‹å¯èƒ½ä½ä¼°äº†å®é™…å€¼'
    else:
        residual_analysis['distribution_shape'] = 'å·¦å'
        residual_analysis['shape_interpretation'] = 'å­˜åœ¨è¾ƒå¤šè´Ÿæ®‹å·®ï¼Œæ¨¡å‹å¯èƒ½é«˜ä¼°äº†å®é™…å€¼'
    
    # å³°åº¦è§£é‡Š
    if residual_analysis['residual_kurtosis'] > 1:
        residual_analysis['kurtosis_interpretation'] = 'æ®‹å·®åˆ†å¸ƒé™¡å³­ï¼Œå­˜åœ¨ç¦»ç¾¤å€¼'
    elif residual_analysis['residual_kurtosis'] < -1:
        residual_analysis['kurtosis_interpretation'] = 'æ®‹å·®åˆ†å¸ƒå¹³å¦ï¼Œæ•°æ®å˜å¼‚æ€§å¤§'
    else:
        residual_analysis['kurtosis_interpretation'] = 'æ®‹å·®åˆ†å¸ƒæ­£å¸¸'
    
    quality['residual_analysis'] = residual_analysis
    
    # æ–°å¢ï¼šæ®‹å·®æ­£æ€åˆ†å¸ƒåˆ†æï¼ˆæ ¸å¿ƒåˆç†æ€§åˆ¤æ–­æ ‡å‡†ï¼‰
    normality_analysis = {
        'jarque_bera_stat': None,
        'jb_p_value': None,
        'ks_stat': None,
        'ks_p_value': None,
        'normal_qq_correlation': None,
        'normality_assessment': 'æœªçŸ¥',
        'normality_interpretation': ''
    }
    
    # è®¡ç®—æ­£æ€åˆ†å¸ƒæ£€éªŒç»Ÿè®¡é‡
    n = len(delta)
    
    # Jarque-Beraæ­£æ€æ€§æ£€éªŒ
    if n >= 20:  # JBæ£€éªŒåœ¨å°æ ·æœ¬æ—¶ä¸å‡†ç¡®
        try:
            from scipy import stats
            # è®¡ç®—JBç»Ÿè®¡é‡å’Œpå€¼
            skewness = residual_analysis['residual_skewness']
            kurtosis = residual_analysis['residual_kurtosis']
            jb_stat = (n / 6) * (skewness**2 + (kurtosis**2) / 4)
            # è‡ªç”±åº¦ä¸º2çš„å¡æ–¹åˆ†å¸ƒ
            jb_p_value = 1 - stats.chi2.cdf(jb_stat, df=2)
            normality_analysis['jarque_bera_stat'] = jb_stat
            normality_analysis['jb_p_value'] = jb_p_value
        except:
            pass
    
    # Kolmogorov-Smirnovæ£€éªŒï¼ˆä¸æ­£æ€åˆ†å¸ƒæ¯”è¾ƒï¼‰
    try:
        from scipy import stats
        # æ ‡å‡†åŒ–æ®‹å·®
        if np.std(delta) > 0:
            z_scores = (delta - np.mean(delta)) / np.std(delta)
            ks_stat, ks_p_value = stats.kstest(z_scores, 'norm')
            normality_analysis['ks_stat'] = ks_stat
            normality_analysis['ks_p_value'] = ks_p_value
    except:
        pass
    
    # Q-Qå›¾ç›¸å…³æ€§æ£€éªŒï¼ˆç®€å•å®ç°ï¼‰
    try:
        # è®¡ç®—ç†è®ºåˆ†ä½æ•°å’Œæ ·æœ¬åˆ†ä½æ•°
        sorted_delta = np.sort(delta)
        n = len(sorted_delta)
        if n > 1:
            # è®¡ç®—ç†è®ºæ­£æ€åˆ†ä½æ•°
            theoretical_quantiles = np.arange(1, n + 1) / (n + 1)
            theoretical_norm = stats.norm.ppf(theoretical_quantiles, loc=np.mean(delta), scale=np.std(delta))
            # è®¡ç®—Q-Qå›¾ä¸Šç‚¹çš„ç›¸å…³æ€§
            qq_corr = np.corrcoef(sorted_delta, theoretical_norm)[0, 1]
            normality_analysis['normal_qq_correlation'] = qq_corr
    except:
        pass
    
    # åŸºäºæ­£æ€åˆ†å¸ƒåˆ†æè¯„ä¼°æ‹Ÿåˆåˆç†æ€§
    normality_assessment = 'è‰¯å¥½'
    normality_interpretations = []
    
    # ä½¿ç”¨å¤šç§æŒ‡æ ‡ç»¼åˆè¯„ä¼°
    # 1. ååº¦å’Œå³°åº¦
    if abs(residual_analysis['residual_skewness']) > 0.5 or abs(residual_analysis['residual_kurtosis']) > 1:
        normality_assessment = 'ä¸€èˆ¬'
        normality_interpretations.append('æ®‹å·®åˆ†å¸ƒçš„ååº¦æˆ–å³°åº¦åç¦»æ­£æ€åˆ†å¸ƒç‰¹å¾')
    
    # 2. Q-Qå›¾ç›¸å…³æ€§
    if 'normal_qq_correlation' in normality_analysis and normality_analysis['normal_qq_correlation'] is not None:
        qq_corr = normality_analysis['normal_qq_correlation']
        if qq_corr < 0.95:
            normality_assessment = 'è¾ƒå·®'
            normality_interpretations.append(f'Q-Qå›¾ç›¸å…³ç³»æ•°ä¸º{qq_corr:.3f}ï¼Œæ®‹å·®åˆ†å¸ƒä¸æ­£æ€åˆ†å¸ƒæœ‰æ˜æ˜¾å·®å¼‚')
    
    # 3. æ­£æ€æ€§æ£€éªŒpå€¼
    p_value_significant = False
    if 'jb_p_value' in normality_analysis and normality_analysis['jb_p_value'] is not None:
        if normality_analysis['jb_p_value'] < 0.05:
            p_value_significant = True
            normality_assessment = 'è¾ƒå·®'
            normality_interpretations.append('Jarque-Beraæ£€éªŒè¡¨æ˜æ®‹å·®æ˜¾è‘—åç¦»æ­£æ€åˆ†å¸ƒ')
    
    if 'ks_p_value' in normality_analysis and normality_analysis['ks_p_value'] is not None:
        if normality_analysis['ks_p_value'] < 0.05:
            p_value_significant = True
            normality_assessment = 'è¾ƒå·®'
            normality_interpretations.append('Kolmogorov-Smirnovæ£€éªŒè¡¨æ˜æ®‹å·®æ˜¾è‘—åç¦»æ­£æ€åˆ†å¸ƒ')
    
    # 4. æ•°æ®ç‚¹åœ¨æ ‡å‡†å·®èŒƒå›´å†…çš„æ¯”ä¾‹ï¼ˆç»éªŒæ³•åˆ™ï¼‰
    if error_analysis['percent_points_within_1std'] < 68 or error_analysis['percent_points_within_2std'] < 95:
        if normality_assessment == 'è‰¯å¥½':
            normality_assessment = 'ä¸€èˆ¬'
        elif normality_assessment == 'ä¸€èˆ¬':
            normality_assessment = 'è¾ƒå·®'
        normality_interpretations.append('æ®‹å·®åˆ†å¸ƒä¸ç¬¦åˆæ­£æ€åˆ†å¸ƒçš„ç»éªŒæ³•åˆ™')
    
    # è®¾ç½®æœ€ç»ˆè¯„ä¼°å’Œè§£é‡Š
    normality_analysis['normality_assessment'] = normality_assessment
    
    if not normality_interpretations:
        normality_interpretations.append('æ®‹å·®åˆ†å¸ƒç¬¦åˆæ­£æ€åˆ†å¸ƒç‰¹å¾ï¼Œæ‹Ÿåˆæ¨¡å‹åˆç†')
    
    normality_analysis['normality_interpretation'] = 'ï¼›'.join(normality_interpretations)
    quality['normality_analysis'] = normality_analysis
    
    # åŸºäºæ®‹å·®æ­£æ€åˆ†å¸ƒåˆ†æé‡æ–°å®šä¹‰æ‹Ÿåˆä¼˜åº¦è¯„ä¼°
    r_squared = poly_fit['r_squared']
    
    # ç»¼åˆè€ƒè™‘RÂ²å’Œæ®‹å·®æ­£æ€æ€§
    if normality_assessment == 'è‰¯å¥½' and r_squared >= 0.9:
        quality['goodness_of_fit'] = 'ä¼˜ç§€'
        quality['goodness_of_fit_interpretation'] = 'æ®‹å·®åˆ†å¸ƒç¬¦åˆæ­£æ€æ€§å‡è®¾ï¼Œä¸”æ¨¡å‹è§£é‡Šäº†90%ä»¥ä¸Šçš„æ•°æ®å˜å¼‚æ€§ï¼Œæ‹Ÿåˆæ•ˆæœç†æƒ³'
    elif normality_assessment == 'è‰¯å¥½' and r_squared >= 0.75:
        quality['goodness_of_fit'] = 'è‰¯å¥½'
        quality['goodness_of_fit_interpretation'] = 'æ®‹å·®åˆ†å¸ƒç¬¦åˆæ­£æ€æ€§å‡è®¾ï¼Œæ¨¡å‹è§£é‡Šäº†75%ä»¥ä¸Šçš„æ•°æ®å˜å¼‚æ€§ï¼Œæ‹Ÿåˆæ•ˆæœè¾ƒå¥½'
    elif normality_assessment == 'ä¸€èˆ¬' and r_squared >= 0.6:
        quality['goodness_of_fit'] = 'ä¸€èˆ¬'
        quality['goodness_of_fit_interpretation'] = 'æ®‹å·®åˆ†å¸ƒåŸºæœ¬ç¬¦åˆæ­£æ€æ€§å‡è®¾ï¼Œæ¨¡å‹è§£é‡Šäº†60%ä»¥ä¸Šçš„æ•°æ®å˜å¼‚æ€§ï¼Œæ‹Ÿåˆæ•ˆæœå¯æ¥å—'
    elif r_squared >= 0.5:
        quality['goodness_of_fit'] = 'è¾ƒå·®'
        quality['goodness_of_fit_interpretation'] = 'æ®‹å·®åˆ†å¸ƒåç¦»æ­£æ€æ€§å‡è®¾æˆ–æ¨¡å‹è§£é‡ŠåŠ›ä¸è¶³ï¼Œæ‹Ÿåˆæ•ˆæœè¾ƒå·®'
    else:
        quality['goodness_of_fit'] = 'å¾ˆå·®'
        quality['goodness_of_fit_interpretation'] = 'æ®‹å·®åˆ†å¸ƒä¸¥é‡åç¦»æ­£æ€æ€§å‡è®¾ä¸”æ¨¡å‹è§£é‡ŠåŠ›æä½ï¼Œæ‹Ÿåˆæ•ˆæœå¾ˆå·®'
    
    # æ–°å¢ï¼šè°ƒæ•´åçš„RÂ²
    k = poly_fit['degree'] + 1  # å‚æ•°æ•°é‡
    if n > k + 1 and r_squared > 0:
        adj_r_squared = 1 - (1 - r_squared) * (n - 1) / (n - k - 1)
        quality['confidence_measures']['adjusted_r_squared'] = adj_r_squared
        # æ·»åŠ è°ƒæ•´åRÂ²çš„è§£é‡Š
        if adj_r_squared > 0.9:
            quality['confidence_measures']['adjusted_r_squared_interpretation'] = 'å³ä½¿è€ƒè™‘æ¨¡å‹å¤æ‚åº¦ï¼Œæ‹Ÿåˆæ•ˆæœä»ç„¶ä¼˜ç§€'
        elif adj_r_squared < r_squared - 0.1:
            quality['confidence_measures']['adjusted_r_squared_interpretation'] = 'æ¨¡å‹å¯èƒ½è¿‡äºå¤æ‚ï¼Œå­˜åœ¨è¿‡æ‹Ÿåˆé£é™©'
    
    # è¯„ä¼°æ•°æ®ä»£è¡¨æ€§ - åŸºäºæ®‹å·®åˆ†æ
    std_delta = np.std(delta)
    mean_abs_delta = np.mean(abs_delta)
    
    # ä½¿ç”¨æ®‹å·®ç»Ÿè®¡é‡è¯„ä¼°æ•°æ®ä»£è¡¨æ€§
    if normality_assessment == 'è‰¯å¥½' and mean_abs_delta < 0.5 * std_delta and r_squared >= 0.85:
        quality['data_representativeness'] = 'éå¸¸å¥½'
        quality['data_representativeness_interpretation'] = 'æ®‹å·®åˆ†å¸ƒæ­£æ€ï¼Œä¸”å¹³å‡ç»å¯¹æ®‹å·®è¾ƒå°ï¼Œæ¨¡å‹å¯¹æ•°æ®çš„ä»£è¡¨æ€§æä½³'
    elif (normality_assessment == 'è‰¯å¥½' or normality_assessment == 'ä¸€èˆ¬') and r_squared >= 0.7:
        quality['data_representativeness'] = 'è‰¯å¥½'
        quality['data_representativeness_interpretation'] = 'æ®‹å·®åˆ†å¸ƒåŸºæœ¬ç¬¦åˆæ­£æ€ï¼Œæ¨¡å‹å¯¹æ•°æ®çš„ä»£è¡¨æ€§è¾ƒå¥½'
    elif r_squared >= 0.5:
        quality['data_representativeness'] = 'ä¸€èˆ¬'
        quality['data_representativeness_interpretation'] = 'æ®‹å·®åˆ†å¸ƒæˆ–æ¨¡å‹è§£é‡ŠåŠ›å­˜åœ¨ä¸è¶³ï¼Œæ¨¡å‹å¯¹æ•°æ®çš„ä»£è¡¨æ€§ä¸€èˆ¬'
    else:
        quality['data_representativeness'] = 'è¾ƒå·®'
        quality['data_representativeness_interpretation'] = 'æ®‹å·®åˆ†å¸ƒä¸¥é‡åç¦»æ­£æ€æˆ–æ¨¡å‹è§£é‡ŠåŠ›ä½ï¼Œæ¨¡å‹å¯¹æ•°æ®çš„ä»£è¡¨æ€§è¾ƒå·®'
    
    # æ–°å¢ï¼šä¸ç¡®å®šåº¦ä¼°è®¡ä¸è§£é‡Š
    std_uncertainty = std_delta / np.sqrt(n) if n > 1 else 0
    quality['uncertainty_estimates'] = {
        'standard_uncertainty': std_uncertainty,
        'expanded_uncertainty_95': 1.96 * std_uncertainty,  # 95%ç½®ä¿¡æ°´å¹³
        'relative_uncertainty': (std_uncertainty / mean_y * 100) if mean_y != 0 else 0
    }
    
    # ä¸ç¡®å®šåº¦ç­‰çº§ä¸è§£é‡Š
    rel_uncertainty = quality['uncertainty_estimates']['relative_uncertainty']
    if rel_uncertainty < 5:
        quality['uncertainty_estimates']['uncertainty_level'] = 'ä½'
        quality['uncertainty_estimates']['uncertainty_interpretation'] = 'æµ‹é‡ç»“æœéå¸¸å¯é '
    elif rel_uncertainty < 10:
        quality['uncertainty_estimates']['uncertainty_level'] = 'ä¸­ä½'
        quality['uncertainty_estimates']['uncertainty_interpretation'] = 'æµ‹é‡ç»“æœè¾ƒä¸ºå¯é '
    elif rel_uncertainty < 20:
        quality['uncertainty_estimates']['uncertainty_level'] = 'ä¸­'
        quality['uncertainty_estimates']['uncertainty_interpretation'] = 'æµ‹é‡ç»“æœæœ‰ä¸€å®šå¯é æ€§ï¼Œä½†éœ€è°¨æ…ä½¿ç”¨'
    else:
        quality['uncertainty_estimates']['uncertainty_level'] = 'é«˜'
        quality['uncertainty_estimates']['uncertainty_interpretation'] = 'æµ‹é‡ç»“æœå¯é æ€§ä½ï¼Œå»ºè®®æ”¹è¿›å®éªŒ'
    
    # å¢å¼ºç‰ˆå»ºè®®ç”Ÿæˆé€»è¾‘ - æ›´å…·é’ˆå¯¹æ€§å’Œå®ç”¨æ€§
    recommendations = []
    detailed_suggestions = []
    
    # 1. åŸºäºæ®‹å·®æ­£æ€åˆ†å¸ƒçš„å»ºè®®
    if normality_assessment == 'è¾ƒå·®':
        recommendations.append('æ®‹å·®åˆ†å¸ƒæ˜¾è‘—åç¦»æ­£æ€ï¼Œæ‹Ÿåˆæ¨¡å‹å‡è®¾å¯èƒ½ä¸æˆç«‹')
        detailed_suggestions.append('å»ºè®®å°è¯•æ•°æ®å˜æ¢(å¦‚å¯¹æ•°ã€å¹³æ–¹æ ¹å˜æ¢)ï¼Œæˆ–è€ƒè™‘ä½¿ç”¨ç¨³å¥å›å½’æ–¹æ³•')
    
    # 2. æ¨¡å‹é€‰æ‹©å»ºè®®
    if poly_fit['degree'] > 5 and r_squared < 0.9:
        recommendations.append(f'å¤šé¡¹å¼é˜¶æ•°({poly_fit["degree"]})è¾ƒé«˜ä½†æ‹Ÿåˆæ•ˆæœä¸€èˆ¬ï¼Œå¯èƒ½å­˜åœ¨è¿‡æ‹Ÿåˆé£é™©')
        detailed_suggestions.append('å»ºè®®å°è¯•ä½¿ç”¨3-4é˜¶å¤šé¡¹å¼ï¼Œæˆ–è€ƒè™‘å¹³æ»‘æ ·æ¡æ‹Ÿåˆä»¥è·å¾—æ›´ç¨³å¥çš„ç»“æœ')
    elif poly_fit['degree'] == 1 and r_squared < 0.7:
        recommendations.append('çº¿æ€§æ‹Ÿåˆæ•ˆæœä¸ä½³ï¼Œæ•°æ®å¯èƒ½å­˜åœ¨éçº¿æ€§å…³ç³»')
        detailed_suggestions.append('å»ºè®®å°è¯•2-3é˜¶å¤šé¡¹å¼æˆ–æŒ‡æ•°/å¯¹æ•°ç­‰éçº¿æ€§æ¨¡å‹')
    elif r_squared < 0.5:
        recommendations.append('æ‹Ÿåˆæ•ˆæœå¾ˆå·®ï¼Œæ¨¡å‹é€‰æ‹©å¯èƒ½ä¸åˆé€‚')
        detailed_suggestions.append('å»ºè®®é‡æ–°è€ƒè™‘æ•°æ®çš„ç‰©ç†/æ•°å­¦æ¨¡å‹ï¼Œæˆ–æ£€æŸ¥æ•°æ®é‡‡é›†è¿‡ç¨‹')
    
    # 3. æ•°æ®è´¨é‡åˆ†æä¸å»ºè®®
    if error_analysis['percent_points_within_1std'] < 70:
        recommendations.append('è¯¯å·®åˆ†å¸ƒè¾ƒå¹¿ï¼Œæ•°æ®è´¨é‡å­˜åœ¨é—®é¢˜')
        detailed_suggestions.append('å»ºè®®æ£€æŸ¥å®éªŒç¯å¢ƒç¨³å®šæ€§ï¼Œæˆ–è€ƒè™‘ä½¿ç”¨é²æ£’æ‹Ÿåˆæ–¹æ³•(å¦‚RANSAC)')
    elif error_analysis['percent_points_within_2std'] < 95:
        recommendations.append('å­˜åœ¨è¾ƒå¤šè¯¯å·®è¾ƒå¤§çš„æ•°æ®ç‚¹')
        detailed_suggestions.append('å»ºè®®ä½¿ç”¨ç®±çº¿å›¾æ–¹æ³•è¯†åˆ«å¼‚å¸¸å€¼ï¼Œæˆ–é‡‡ç”¨å±€éƒ¨åŠ æƒå›å½’æ–¹æ³•')
    
    # 4. æ•°æ®é‡ä¸åˆ†å¸ƒå»ºè®®
    if len(x_data) < 10:
        recommendations.append(f'æ•°æ®ç‚¹æ•°é‡è¾ƒå°‘({len(x_data)}ä¸ª)ï¼Œç»Ÿè®¡å¯é æ€§ä¸è¶³')
        detailed_suggestions.append('å»ºè®®è‡³å°‘å¢åŠ åˆ°15-20ä¸ªæ•°æ®ç‚¹ï¼Œç¡®ä¿è¦†ç›–æ•´ä¸ªå˜é‡èŒƒå›´')
    elif len(x_data) < 5 and poly_fit['degree'] > 2:
        recommendations.append('æ•°æ®ç‚¹è¿‡å°‘è€Œæ¨¡å‹è¿‡äºå¤æ‚')
        detailed_suggestions.append('å»ºè®®å¢åŠ 3å€äºå¤šé¡¹å¼é˜¶æ•°çš„æ•°æ®ç‚¹ï¼Œæˆ–é™ä½æ¨¡å‹å¤æ‚åº¦')
    
    # 5. ç³»ç»Ÿæ€§è¯¯å·®æ£€æµ‹
    if abs(residual_analysis['residual_mean']) > 0.5 * residual_analysis['residual_std']:
        recommendations.append('æ®‹å·®å‡å€¼æ˜æ˜¾åç¦»é›¶ï¼Œå­˜åœ¨ç³»ç»Ÿæ€§è¯¯å·®')
        detailed_suggestions.append('å»ºè®®æ£€æŸ¥æµ‹é‡ä»ªå™¨æ ¡å‡†çŠ¶æ€ï¼Œæˆ–è€ƒè™‘æ¨¡å‹ä¸­åŠ å…¥å¸¸æ•°é¡¹')
    elif abs(residual_analysis['residual_skewness']) > 1:
        recommendations.append('æ®‹å·®åˆ†å¸ƒæ˜æ˜¾åæ–œï¼Œæ¨¡å‹å‡è®¾å¯èƒ½ä¸æˆç«‹')
        detailed_suggestions.append('å»ºè®®å°è¯•æ•°æ®å˜æ¢(å¦‚å¯¹æ•°å˜æ¢)æˆ–ä½¿ç”¨éå‚æ•°å›å½’æ–¹æ³•')
    
    # 6. å®éªŒæ”¹è¿›å»ºè®®
    if rel_uncertainty > 20:
        recommendations.append('ç›¸å¯¹ä¸ç¡®å®šåº¦è¾ƒå¤§ï¼Œå®éªŒå¯é æ€§ä½')
        detailed_suggestions.append('å»ºè®®æ”¹è¿›æµ‹é‡æ–¹æ³•ï¼Œå¢åŠ é‡å¤æµ‹é‡æ¬¡æ•°ï¼Œæ§åˆ¶ç¯å¢ƒå˜é‡')
    elif rel_uncertainty > 10:
        recommendations.append('ç›¸å¯¹ä¸ç¡®å®šåº¦ä¸­ç­‰ï¼Œå®éªŒå¯é æ€§ä¸€èˆ¬')
        detailed_suggestions.append('å»ºè®®åœ¨å…³é”®æ•°æ®ç‚¹å¢åŠ é‡å¤æµ‹é‡ï¼Œæé«˜ä»ªå™¨ç²¾åº¦')
    
    # 7. æ•°æ®å¤„ç†ä¼˜åŒ–å»ºè®®
    if quality['data_representativeness'] in ['ä¸€èˆ¬', 'è¾ƒå·®']:
        recommendations.append('æ¨¡å‹å¯¹æ•°æ®çš„ä»£è¡¨æ€§ä¸è¶³')
        detailed_suggestions.append('å»ºè®®è€ƒè™‘åˆ†æ®µæ‹Ÿåˆï¼Œæˆ–ä½¿ç”¨è‡ªé€‚åº”æ‹Ÿåˆæ–¹æ³•')
    
    # 8. ç‰¹æ®Šæƒ…å†µè¯¦ç»†åˆ†æ
    if 'mean_relative_error' in error_analysis and error_analysis['mean_relative_error'] > 30:
        recommendations.append('å¹³å‡ç›¸å¯¹è¯¯å·®è¿‡å¤§ï¼Œé¢„æµ‹ç²¾åº¦ä½')
        detailed_suggestions.append('å»ºè®®é‡æ–°æ£€æŸ¥æ•°æ®é‡‡é›†è¿‡ç¨‹ï¼Œæˆ–è€ƒè™‘ä½¿ç”¨æ›´é€‚åˆçš„æ•°å­¦æ¨¡å‹')
    
    # 9. æ•°æ®åˆ†å¸ƒå‡åŒ€æ€§å»ºè®®
    x_range = max(x_data) - min(x_data)
    x_spacing = np.diff(np.sort(x_data))
    if x_range > 0 and np.max(x_spacing) > 3 * np.mean(x_spacing):
        recommendations.append('æ•°æ®ç‚¹åˆ†å¸ƒä¸å‡åŒ€ï¼Œå¯èƒ½å½±å“æ‹Ÿåˆè´¨é‡')
        detailed_suggestions.append('å»ºè®®åœ¨æ•°æ®å¯†é›†åŒºåŸŸé€‚å½“å‡å°‘ç‚¹ï¼Œç¨€ç–åŒºåŸŸå¢åŠ ç‚¹ï¼Œä½¿æ•°æ®åˆ†å¸ƒæ›´å‡åŒ€')
    
    # 10. æ®‹å·®è‡ªç›¸å…³æ€§æ£€æŸ¥ï¼ˆç®€å•å®ç°ï¼‰
    if len(delta) > 5:
        # è®¡ç®—ä¸€é˜¶è‡ªç›¸å…³
        lag1_residuals = delta[:-1]
        lag1_next = delta[1:]
        corr_coef = np.corrcoef(lag1_residuals, lag1_next)[0, 1]
        if abs(corr_coef) > 0.5:
            recommendations.append('æ®‹å·®å­˜åœ¨æ˜æ˜¾è‡ªç›¸å…³æ€§')
            detailed_suggestions.append('æ•°æ®å¯èƒ½å­˜åœ¨æ—¶é—´åºåˆ—ç‰¹æ€§ï¼Œå»ºè®®è€ƒè™‘æ—¶é—´åºåˆ—æ¨¡å‹æˆ–è°ƒæ•´å®éªŒé¡ºåº')
    
    # æ•´åˆå»ºè®®
    quality['recommendations'] = recommendations
    quality['detailed_suggestions'] = detailed_suggestions
    
    return quality

def format_experiment_results(results: Dict[str, Any]) -> str:
    """ä¼˜åŒ–ç‰ˆæ ¼å¼åŒ–å®éªŒæ¨¡å¼çš„ç»“æœè¾“å‡ºï¼Œæä¾›æ›´å®ç”¨çš„æ•°æ®åˆ†æå’Œå¯è§†åŒ–ä¿¡æ¯"""
    output = []
    
    # æ•°æ®æ¦‚è§ˆï¼ˆä¼˜åŒ–ç‰ˆï¼‰- çªå‡ºå…³é”®ä¿¡æ¯
    output.append("ğŸ“Š å®éªŒæ•°æ®å¤„ç†ç»“æœ ğŸ“Š")
    output.append("=" * 45)
    output.append(f"åŸå§‹æ•°æ®ç‚¹æ•°é‡: {len(results['original_data'][0])}")
    output.append(f"è¿‡æ»¤åæ•°æ®ç‚¹æ•°é‡: {len(results['filtered_data'][0])}")
    
    if len(results['filtered_indices']) > 0:
        output.append(f"è¿‡æ»¤çš„å¼‚å¸¸ç‚¹æ•°é‡: {len(results['filtered_indices'])}")
        # è®¡ç®—å¼‚å¸¸ç‚¹å æ¯”
        outlier_percent = len(results['filtered_indices']) / len(results['original_data'][0]) * 100
        output.append(f"å¼‚å¸¸ç‚¹å æ¯”: {outlier_percent:.1f}%")
        # æ·»åŠ å¼‚å¸¸ç‚¹è¯„ä»·
        if outlier_percent < 5:
            output.append(f"âœ… å¼‚å¸¸ç‚¹è¯„ä»·: æ•°æ®è´¨é‡è‰¯å¥½ï¼Œå¼‚å¸¸ç‚¹è¾ƒå°‘")
        elif outlier_percent < 10:
            output.append(f"âš ï¸ å¼‚å¸¸ç‚¹è¯„ä»·: å­˜åœ¨å°‘é‡å¼‚å¸¸ç‚¹")
        elif outlier_percent < 20:
            output.append(f"âš ï¸ å¼‚å¸¸ç‚¹è¯„ä»·: å¼‚å¸¸ç‚¹æ¯”ä¾‹ä¸­ç­‰ï¼Œå»ºè®®æ£€æŸ¥åŸå§‹æ•°æ®")
        else:
            output.append(f"âŒ å¼‚å¸¸ç‚¹è¯„ä»·: å¼‚å¸¸ç‚¹æ¯”ä¾‹è¾ƒé«˜ï¼Œæ•°æ®å¯é æ€§å¯èƒ½å—å½±å“")
    else:
        output.append("âœ… å¼‚å¸¸ç‚¹è¯„ä»·: æœªæ£€æµ‹åˆ°å¼‚å¸¸ç‚¹")
    
    if results['threshold_used'] > 0:
        output.append(f"ä½¿ç”¨çš„å¼‚å¸¸ç‚¹è¿‡æ»¤é˜ˆå€¼: {results['threshold_used']}å€æ ‡å‡†å·®")
    
    # å¢å¼ºçš„ç»Ÿè®¡ä¿¡æ¯ - ç²¾ç®€ä½†ä¿ç•™å…³é”®æŒ‡æ ‡
    output.append("\nğŸ“ˆ è¿‡æ»¤åæ•°æ®ç»Ÿè®¡ä¿¡æ¯ ğŸ“ˆ")
    output.append("=" * 45)
    stats = results['filtered_stats']
    if stats:
        # åŸºç¡€ç»Ÿè®¡é‡
        output.append(f"æ•°æ®ç‚¹æ•°é‡: {stats['n_points']}")
        
        # Xæ•°æ®ç»Ÿè®¡ - ä¿ç•™æ ¸å¿ƒæŒ‡æ ‡
        output.append("\nXè½´æ•°æ®ç»Ÿè®¡:")
        output.append(f"  èŒƒå›´: [{stats['min_x']:.4f}, {stats['max_x']:.4f}]")
        output.append(f"  ä¸­ä½æ•°: {stats['median_x']:.4f}")
        output.append(f"  å¹³å‡å€¼: {stats['mean_x']:.4f}")
        output.append(f"  æ ‡å‡†å·®: {stats['std_x']:.4f}")
        
        # è®¡ç®—Xçš„å˜å¼‚ç³»æ•°
        cv_x = stats.get('cv_x', (stats['std_x'] / stats['mean_x'] * 100) if stats['mean_x'] != 0 else 0)
        output.append(f"  å˜å¼‚ç³»æ•°: {cv_x:.2f}%")
        # å˜å¼‚ç¨‹åº¦è¯„ä»·å·²ç§»é™¤
        
        # Yæ•°æ®ç»Ÿè®¡ - ä¿ç•™æ ¸å¿ƒæŒ‡æ ‡
        output.append("\nYè½´æ•°æ®ç»Ÿè®¡:")
        output.append(f"  èŒƒå›´: [{stats['min_y']:.4f}, {stats['max_y']:.4f}]")
        output.append(f"  ä¸­ä½æ•°: {stats['median_y']:.4f}")
        output.append(f"  å¹³å‡å€¼: {stats['mean_y']:.4f}")
        output.append(f"  æ ‡å‡†å·®: {stats['std_y']:.4f}")
        
        # è®¡ç®—Yçš„å˜å¼‚ç³»æ•°
        cv_y = stats.get('cv_y', (stats['std_y'] / stats['mean_y'] * 100) if stats['mean_y'] != 0 else 0)
        output.append(f"  å˜å¼‚ç³»æ•°: {cv_y:.2f}%")
        # å˜å¼‚ç¨‹åº¦è¯„ä»·å·²ç§»é™¤
        
        # ç›¸å…³æ€§åˆ†æ
        if 'correlation' in stats:
            corr = stats['correlation']
            output.append("\nğŸ”— ç›¸å…³æ€§åˆ†æ:")
            output.append(f"  X-Yç›¸å…³ç³»æ•°: {corr:.4f}")
            
            # ç›¸å…³æ€§è§£é‡Š
            if abs(corr) >= 0.9:
                corr_interpretation = "å¼ºç›¸å…³"
                icon = "âœ…"
            elif abs(corr) >= 0.7:
                corr_interpretation = "ä¸­åº¦å¼ºç›¸å…³"
                icon = "âœ…"
            elif abs(corr) >= 0.5:
                corr_interpretation = "ä¸­åº¦ç›¸å…³"
                icon = "âš ï¸"
            elif abs(corr) >= 0.3:
                corr_interpretation = "å¼±ç›¸å…³"
                icon = "âš ï¸"
            else:
                corr_interpretation = "æå¼±ç›¸å…³æˆ–æ— ç›¸å…³"
                icon = "âŒ"
            
            direction = "æ­£ç›¸å…³" if corr > 0 else "è´Ÿç›¸å…³" if corr < 0 else "æ— ç›¸å…³"
            output.append(f"  {icon} ç›¸å…³æ€§è§£é‡Š: {corr_interpretation} ({direction})")
            
            # ç›¸å…³æ€§å¼ºåº¦è¯„ä»·
            if abs(corr) >= 0.7:
                output.append(f"  ğŸ’¡ ç›¸å…³æ€§å¼ºåº¦è¯„ä»·: ä¸¤å˜é‡å…³ç³»å¯†åˆ‡ï¼Œé€‚åˆä½¿ç”¨å¤šé¡¹å¼æ‹Ÿåˆ")
            elif abs(corr) >= 0.3:
                output.append(f"  ğŸ’¡ ç›¸å…³æ€§å¼ºåº¦è¯„ä»·: ä¸¤å˜é‡å­˜åœ¨ä¸€å®šå…³ç³»ï¼Œæ‹Ÿåˆå¯èƒ½å­˜åœ¨ä¸­ç­‰è¯¯å·®")
            else:
                output.append(f"  ğŸ’¡ ç›¸å…³æ€§å¼ºåº¦è¯„ä»·: ä¸¤å˜é‡å…³ç³»è¾ƒå¼±ï¼Œå»ºè®®è€ƒè™‘å…¶ä»–å»ºæ¨¡æ–¹æ³•æˆ–å¢åŠ æ•°æ®é‡")
    
    # æœ€ä½³å¤šé¡¹å¼æ‹Ÿåˆï¼ˆä¼˜åŒ–å±•ç¤ºï¼‰
    if results['best_poly_fit']:
        output.append("\nğŸ” æœ€ä½³å¤šé¡¹å¼æ‹Ÿåˆç»“æœ ğŸ”")
        output.append("=" * 45)
        poly = results['best_poly_fit']
        output.append(f"å¤šé¡¹å¼é˜¶æ•°: {poly['degree']}")
        
        # æ ¼å¼åŒ–å¤šé¡¹å¼è¡¨è¾¾å¼ï¼ˆæ›´æ˜“è¯»çš„æ ¼å¼ï¼‰
        coeffs = poly['coeffs']
        terms = []
        for i, coef in enumerate(reversed(coeffs)):
            if abs(coef) < 1e-10:  # è·³è¿‡æ¥è¿‘é›¶çš„ç³»æ•°
                continue
            
            if i == 0:
                terms.append(f"{coef:+.4f}")
            elif i == 1:
                terms.append(f"{coef:+.4f}x")
            else:
                terms.append(f"{coef:+.4f}x^{i}")
        
        # ç§»é™¤ç¬¬ä¸€ä¸ªé¡¹çš„+å·
        if terms and terms[0].startswith('+'):
            terms[0] = terms[0][1:]
        
        polynomial_str = " ".join(terms)
        output.append(f"å¤šé¡¹å¼æ–¹ç¨‹: y = {polynomial_str}")
        
        # æ‹Ÿåˆè´¨é‡æŒ‡æ ‡ - çªå‡ºå…³é”®æŒ‡æ ‡
        output.append("\nğŸ“Š æ‹Ÿåˆè´¨é‡æŒ‡æ ‡:")
        output.append(f"  å‡æ–¹æ ¹è¯¯å·®(RMSE): {poly['rmse']:.6f}")
        # å¢åŠ MAEå¦‚æœå¯ç”¨
        if 'mae' in poly:
            output.append(f"  å¹³å‡ç»å¯¹è¯¯å·®(MAE): {poly['mae']:.6f}")
        output.append(f"  å†³å®šç³»æ•°(RÂ²): {poly['r_squared']:.4f}")
        
        if 'adjusted_r_squared' in poly:
            output.append(f"  è°ƒæ•´åçš„å†³å®šç³»æ•°(Adj-RÂ²): {poly['adjusted_r_squared']:.4f}")
        
        # æ¨¡å‹å¤æ‚åº¦è¯„ä¼°
        if poly['degree'] <= 2:
            output.append(f"  âœ… æ¨¡å‹å¤æ‚åº¦: ä½ - æ¨¡å‹ç®€å•ï¼Œæ³›åŒ–èƒ½åŠ›å¼º")
        elif poly['degree'] <= 4:
            output.append(f"  âš ï¸ æ¨¡å‹å¤æ‚åº¦: ä¸­ç­‰ - å¹³è¡¡æ‹Ÿåˆç²¾åº¦å’Œæ³›åŒ–èƒ½åŠ›")
        else:
            output.append(f"  âŒ æ¨¡å‹å¤æ‚åº¦: é«˜ - æ‹Ÿåˆç²¾åº¦é«˜ä½†å¯èƒ½è¿‡æ‹Ÿåˆ")
        
        # AICå’ŒBICä¿¡æ¯å‡†åˆ™ - ä»…ä¿ç•™å…³é”®ä¿¡æ¯
        if 'aic' in poly:
            output.append(f"  AICä¿¡æ¯å‡†åˆ™: {poly['aic']:.4f}")
        if 'bic' in poly:
            output.append(f"  BICä¿¡æ¯å‡†åˆ™: {poly['bic']:.4f}")
        
        # è¿‡æ‹Ÿåˆé£é™©è¯„ä¼°
        if 'adjusted_r_squared' in poly and 'r_squared' in poly:
            r_diff = poly['r_squared'] - poly['adjusted_r_squared']
            if r_diff > 0.1:
                output.append(f"  âŒ è¿‡æ‹Ÿåˆé£é™©: é«˜ (RÂ²ä¸Adj-RÂ²å·®å¼‚è¾ƒå¤§)ï¼Œå»ºè®®é™ä½å¤šé¡¹å¼é˜¶æ•°")
            elif r_diff > 0.05:
                output.append(f"  âš ï¸ è¿‡æ‹Ÿåˆé£é™©: ä¸­ç­‰ï¼Œå¯è€ƒè™‘éªŒè¯é›†æµ‹è¯•")
            else:
                output.append(f"  âœ… è¿‡æ‹Ÿåˆé£é™©: ä½ï¼Œæ¨¡å‹æ³›åŒ–èƒ½åŠ›è‰¯å¥½")
    
    # å¹³æ»‘æ ·æ¡æ‹Ÿåˆç»“æœï¼ˆå¦‚æœå­˜åœ¨ï¼‰- å¢å¼ºå±•ç¤º
    if 'smooth_curve' in results and results['smooth_curve']:
        output.append("\nğŸ“ˆ å¹³æ»‘æ ·æ¡æ‹Ÿåˆç»“æœ ğŸ“ˆ")
        output.append("=" * 45)
        output.append("âœ… å¹³æ»‘æ ·æ¡æ‹Ÿåˆå·²å®Œæˆï¼Œé€‚åˆå±•ç¤ºæ•°æ®è¶‹åŠ¿å˜åŒ–")
        
        # è®¡ç®—å¹³æ»‘æ›²çº¿ä¸åŸå§‹æ•°æ®çš„è¯¯å·®
        if results['best_poly_fit']:
            # åŸºäºå¤šé¡¹å¼æ‹Ÿåˆçš„è¯¯å·®è¿›è¡Œå‚è€ƒæ¯”è¾ƒ
            poly_mse = results['best_poly_fit']['mse']
            # è®¡ç®—å¹³æ»‘æ›²çº¿çš„MSE
            x_data, y_data = results['filtered_data']
            smooth_x, smooth_y = results['smooth_curve']
            # å¯¹äºæ¯ä¸ªè¿‡æ»¤åçš„æ•°æ®ç‚¹ï¼Œæ‰¾åˆ°å¯¹åº”çš„å¹³æ»‘æ›²çº¿ä¸Šçš„yå€¼ï¼ˆä½¿ç”¨æœ€è¿‘çš„xå€¼ï¼‰
            smooth_y_for_data = []
            for x in x_data:
                # æ‰¾åˆ°æœ€æ¥è¿‘xçš„smooth_xå€¼çš„ç´¢å¼•
                idx = np.argmin(np.abs(smooth_x - x))
                smooth_y_for_data.append(smooth_y[idx])
            
            # è®¡ç®—å¹³æ»‘æ›²çº¿çš„MSE
            smooth_mse = np.mean((np.array(y_data) - np.array(smooth_y_for_data))**2)
            output.append(f"  ğŸ’¡ å¤šé¡¹å¼æ‹ŸåˆMSE: {poly_mse:.6f}")
            output.append(f"  ğŸ’¡ å¹³æ»‘æ ·æ¡æ‹ŸåˆMSE: {smooth_mse:.6f}")
            
            # æ¯”è¾ƒä¸¤ç§æ‹Ÿåˆæ–¹æ³•
            if smooth_mse < poly_mse:
                output.append(f"  âœ… å¹³æ»‘æ ·æ¡æ‹Ÿåˆæ•ˆæœæ›´å¥½ï¼Œæä¾›äº†æ›´çµæ´»çš„æ•°æ®è¶‹åŠ¿è¡¨ç¤º")
            else:
                output.append(f"  âš ï¸ å¤šé¡¹å¼æ‹Ÿåˆç²¾åº¦æ›´é«˜ï¼Œä½†å¹³æ»‘æ ·æ¡å¯èƒ½æ›´å¥½åœ°æ•æ‰éçº¿æ€§è¶‹åŠ¿")
    
    # å¢å¼ºçš„æ›²çº¿è´¨é‡è¯„ä¼°
    output.append("\nğŸŒŸ æ›²çº¿è´¨é‡ç»¼åˆè¯„ä¼° ğŸŒŸ")
    output.append("=" * 45)
    quality = results['curve_quality']
    output.append(f"æ‹Ÿåˆä¼˜åº¦ç­‰çº§: {quality['goodness_of_fit']}")
    output.append(f"æ•°æ®ä»£è¡¨æ€§: {quality['data_representativeness']}")
    
    # è¯¦ç»†è¯¯å·®åˆ†æ - ç²¾ç®€ä½†ä¿ç•™å…³é”®æŒ‡æ ‡
    error_analysis = quality['error_analysis']
    if error_analysis:
        output.append("\nâš ï¸ è¯¦ç»†è¯¯å·®åˆ†æ âš ï¸")
        output.append("=" * 45)
        output.append(f"å¹³å‡ç»å¯¹è¯¯å·®(MAE): {error_analysis['mean_absolute_error']:.6f}")
        output.append(f"æœ€å¤§ç»å¯¹è¯¯å·®: {error_analysis['max_absolute_error']:.6f}")
        output.append(f"æ ‡å‡†è¯¯å·®: {error_analysis['std_error']:.6f}")
        
        # ç›¸å¯¹è¯¯å·®ä¿¡æ¯ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if 'mean_relative_error' in error_analysis:
            output.append(f"å¹³å‡ç›¸å¯¹è¯¯å·®: {error_analysis['mean_relative_error']:.2f}%")
            output.append(f"æœ€å¤§ç›¸å¯¹è¯¯å·®: {error_analysis['max_relative_error']:.2f}%")
            
            # ç›¸å¯¹è¯¯å·®è¯„ä»·
            mean_rel_error = error_analysis['mean_relative_error']
            if mean_rel_error < 5:
                output.append(f"âœ… ç›¸å¯¹è¯¯å·®è¯„ä»·: ä¼˜ç§€ - è¯¯å·®åœ¨å¯æ¥å—èŒƒå›´å†…")
            elif mean_rel_error < 10:
                output.append(f"âœ… ç›¸å¯¹è¯¯å·®è¯„ä»·: è‰¯å¥½ - è¯¯å·®è¾ƒå°")
            elif mean_rel_error < 20:
                output.append(f"âš ï¸ ç›¸å¯¹è¯¯å·®è¯„ä»·: ä¸€èˆ¬ - è¯¯å·®ä¸­ç­‰ï¼Œå¯æ¥å—")
            elif mean_rel_error < 30:
                output.append(f"âŒ ç›¸å¯¹è¯¯å·®è¯„ä»·: è¾ƒå·® - è¯¯å·®è¾ƒå¤§ï¼Œå»ºè®®æ”¹è¿›")
            else:
                output.append(f"âŒ ç›¸å¯¹è¯¯å·®è¯„ä»·: å¾ˆå·® - è¯¯å·®è¿‡å¤§ï¼Œéœ€é‡æ–°å»ºæ¨¡")
        
        # è¯¯å·®åˆ†å¸ƒç»Ÿè®¡
        output.append("\nè¯¯å·®åˆ†å¸ƒæƒ…å†µ:")
        output.append(f"  1å€æ ‡å‡†å·®å†…çš„æ•°æ®ç‚¹: {error_analysis['percent_points_within_1std']:.1f}%")
        output.append(f"  2å€æ ‡å‡†å·®å†…çš„æ•°æ®ç‚¹: {error_analysis['percent_points_within_2std']:.1f}%")
        
        # è¯¯å·®åˆ†å¸ƒè¯„ä»·
        percent_within_1std = error_analysis['percent_points_within_1std']
        percent_within_2std = error_analysis['percent_points_within_2std']
        
        if percent_within_1std >= 90:
            output.append(f"  âœ… è¯¯å·®åˆ†å¸ƒè¯„ä»·: éå¸¸é›†ä¸­ï¼Œæ‹Ÿåˆç¨³å®šæ€§é«˜")
        elif percent_within_1std >= 70:
            output.append(f"  âš ï¸ è¯¯å·®åˆ†å¸ƒè¯„ä»·: ç›¸å¯¹é›†ä¸­ï¼Œæ‹Ÿåˆè¾ƒç¨³å®š")
        elif percent_within_2std >= 90:
            output.append(f"  âš ï¸ è¯¯å·®åˆ†å¸ƒè¯„ä»·: ä¸€èˆ¬ï¼Œæ‹Ÿåˆç¨³å®šæ€§ä¸€èˆ¬")
        else:
            output.append(f"  âŒ è¯¯å·®åˆ†å¸ƒè¯„ä»·: åˆ†æ•£ï¼Œæ‹Ÿåˆä¸ç¨³å®šï¼Œå»ºè®®æ£€æŸ¥å¼‚å¸¸ç‚¹æˆ–å¢åŠ æ•°æ®é‡")
    
    # æ®‹å·®åˆ†æ - ä¼˜åŒ–ç‰ˆï¼Œæ›´ç®€æ´æ˜“è¯»
    if 'residual_analysis' in quality and quality['residual_analysis']:
        residual_analysis = quality['residual_analysis']
        output.append("\nğŸ“Š æ®‹å·®åˆ†æ ğŸ“Š")
        output.append("=" * 45)
        output.append(f"æ®‹å·®å‡å€¼: {residual_analysis['residual_mean']:.6f}")
        output.append(f"æ®‹å·®æ ‡å‡†å·®: {residual_analysis['residual_std']:.6f}")
        output.append(f"æ®‹å·®ååº¦: {residual_analysis['residual_skewness']:.3f}")
        output.append(f"æ®‹å·®å³°åº¦: {residual_analysis['residual_kurtosis']:.3f}")
        
        if 'distribution_shape' in residual_analysis:
            output.append(f"æ®‹å·®åˆ†å¸ƒå½¢çŠ¶: {residual_analysis['distribution_shape']}")
        
        # æ®‹å·®æ­£æ€æ€§åˆ¤æ–­å’Œå»ºè®®
        if abs(residual_analysis['residual_mean']) < 1e-6:
            output.append("âœ… æ®‹å·®å‡å€¼æ¥è¿‘é›¶ï¼Œç¬¦åˆæ¨¡å‹å‡è®¾ï¼Œæ‹Ÿåˆæ— ç³»ç»Ÿæ€§åå·®")
        else:
            output.append("âŒ æ®‹å·®å‡å€¼åç¦»é›¶ï¼Œå­˜åœ¨ç³»ç»Ÿæ€§åå·®ï¼Œå»ºè®®æ£€æŸ¥æ¨¡å‹å‡è®¾æˆ–è€ƒè™‘å…¶ä»–æ¨¡å‹")
        
        # æ®‹å·®ååº¦è§£é‡Š
        skewness = residual_analysis['residual_skewness']
        if abs(skewness) < 0.5:
            output.append(f"âœ… æ®‹å·®ååº¦é€‚ä¸­ï¼Œåˆ†å¸ƒæ¥è¿‘å¯¹ç§°ï¼Œæ‹Ÿåˆæ•ˆæœè‰¯å¥½")
        elif skewness > 0:
            output.append(f"âš ï¸ æ®‹å·®æ˜¾è‘—å³åï¼Œæ¨¡å‹å¯¹ä½å€¼ä¼°è®¡è¾ƒå¥½ï¼Œé«˜å€¼ä¼°è®¡åä½")
        else:
            output.append(f"âš ï¸ æ®‹å·®æ˜¾è‘—å·¦åï¼Œæ¨¡å‹å¯¹é«˜å€¼ä¼°è®¡è¾ƒå¥½ï¼Œä½å€¼ä¼°è®¡åé«˜")
    
    # æ–°å¢ï¼šæ®‹å·®æ­£æ€åˆ†å¸ƒåˆ†æï¼ˆåŸºäºdeltaçš„åˆç†æ€§åˆ¤æ–­ï¼‰
    if 'normality_analysis' in quality and quality['normality_analysis']:
        normality_analysis = quality['normality_analysis']
        output.append("\nğŸ”¬ æ®‹å·®æ­£æ€åˆ†å¸ƒåˆ†æ ğŸ”¬")
        output.append("=" * 45)
        
        # æ˜¾ç¤ºæ­£æ€æ€§è¯„ä¼°ç»“æœï¼ˆæ ¸å¿ƒåˆç†æ€§åˆ¤æ–­ï¼‰
        normality_icon = "âœ…" if normality_analysis['normality_assessment'] == "è‰¯å¥½" else "âš ï¸" if normality_analysis['normality_assessment'] == "ä¸€èˆ¬" else "âŒ"
        output.append(f"{normality_icon} æ­£æ€æ€§è¯„ä¼°ç­‰çº§: {normality_analysis['normality_assessment']}")
        output.append(f"ğŸ“ è¯„ä¼°è§£é‡Š: {normality_analysis['normality_interpretation']}")
        
        # æ˜¾ç¤ºç»Ÿè®¡æ£€éªŒç»“æœï¼ˆå¦‚æœæœ‰ï¼‰
        test_results = []
        if normality_analysis.get('jarque_bera_stat') is not None:
            jb_stat = normality_analysis['jarque_bera_stat']
            jb_p_value = normality_analysis['jb_p_value']
            jb_icon = "âœ…" if jb_p_value >= 0.05 else "âŒ"
            test_results.append(f"{jb_icon} Jarque-Beraç»Ÿè®¡é‡: {jb_stat:.4f}, på€¼: {jb_p_value:.4f}")
        
        if normality_analysis.get('ks_stat') is not None:
            ks_stat = normality_analysis['ks_stat']
            ks_p_value = normality_analysis['ks_p_value']
            ks_icon = "âœ…" if ks_p_value >= 0.05 else "âŒ"
            test_results.append(f"{ks_icon} Kolmogorov-Smirnovç»Ÿè®¡é‡: {ks_stat:.4f}, på€¼: {ks_p_value:.4f}")
        
        if normality_analysis.get('normal_qq_correlation') is not None:
            qq_corr = normality_analysis['normal_qq_correlation']
            qq_icon = "âœ…" if qq_corr >= 0.95 else "âš ï¸" if qq_corr >= 0.9 else "âŒ"
            test_results.append(f"{qq_icon} Q-Qå›¾ç›¸å…³ç³»æ•°: {qq_corr:.4f}")
        
        # å¦‚æœæœ‰ç»Ÿè®¡æ£€éªŒç»“æœï¼Œè¾“å‡ºå®ƒä»¬
        if test_results:
            output.append("\nğŸ“Š æ­£æ€æ€§æ£€éªŒç»“æœ:")
            for result in test_results:
                output.append(f"  {result}")
        
        # æ ¹æ®æ­£æ€æ€§è¯„ä¼°æä¾›ç‰¹å®šå»ºè®®
        if normality_analysis['normality_assessment'] == "è‰¯å¥½":
            output.append("âœ… åˆç†æ€§åˆ¤æ–­: æ®‹å·®åˆ†å¸ƒç¬¦åˆæ­£æ€æ€§å‡è®¾ï¼Œæ‹Ÿåˆæ¨¡å‹éå¸¸åˆç†")
        elif normality_analysis['normality_assessment'] == "ä¸€èˆ¬":
            output.append("âš ï¸ åˆç†æ€§åˆ¤æ–­: æ®‹å·®åˆ†å¸ƒåŸºæœ¬ç¬¦åˆæ­£æ€æ€§å‡è®¾ï¼Œæ‹Ÿåˆæ¨¡å‹åˆç†ä½†å­˜åœ¨æ”¹è¿›ç©ºé—´")
        else:
            output.append("âŒ åˆç†æ€§åˆ¤æ–­: æ®‹å·®åˆ†å¸ƒæ˜¾è‘—åç¦»æ­£æ€æ€§å‡è®¾ï¼Œæ‹Ÿåˆæ¨¡å‹åˆç†æ€§è¾ƒå·®ï¼Œå»ºè®®æ”¹è¿›")
    
    # ä¸ç¡®å®šåº¦åˆ†æ - ä¼˜åŒ–ç‰ˆï¼Œæ›´ç®€æ´æ˜“è¯»
    if 'uncertainty_estimates' in quality and quality['uncertainty_estimates']:
        uncertainty = quality['uncertainty_estimates']
        output.append("\nğŸ” ä¸ç¡®å®šåº¦åˆ†æ ğŸ”")
        output.append("=" * 45)
        output.append(f"æ ‡å‡†ä¸ç¡®å®šåº¦: {uncertainty['standard_uncertainty']:.6f}")
        output.append(f"95%ç½®ä¿¡æ°´å¹³æ‰©å±•ä¸ç¡®å®šåº¦: {uncertainty['expanded_uncertainty_95']:.6f}")
        
        if 'relative_uncertainty' in uncertainty:
            output.append(f"ç›¸å¯¹ä¸ç¡®å®šåº¦: {uncertainty['relative_uncertainty']:.2f}%")
            
            # ä¸ç¡®å®šåº¦ç­‰çº§å’Œè§£é‡Š - æ›´ç®€æ´çš„è§£é‡Š
            rel_uncertainty = uncertainty['relative_uncertainty']
            if rel_uncertainty < 5:
                output.append("âœ… ä¸ç¡®å®šåº¦ç­‰çº§: ä½")
                output.append("ğŸ’¡ æ•°æ®å¤„ç†å»ºè®®: å½“å‰æ•°æ®å¤„ç†æ–¹æ³•å¯é ï¼Œå¯ä¿æŒç°æœ‰å‚æ•°")
            elif rel_uncertainty < 10:
                output.append("âœ… ä¸ç¡®å®šåº¦ç­‰çº§: ä¸­ä½")
                output.append("ğŸ’¡ æ•°æ®å¤„ç†å»ºè®®: å¯è€ƒè™‘è½»å¾®ä¼˜åŒ–å‚æ•°ä»¥è¿›ä¸€æ­¥é™ä½ä¸ç¡®å®šåº¦")
            elif rel_uncertainty < 20:
                output.append("âš ï¸ ä¸ç¡®å®šåº¦ç­‰çº§: ä¸­ç­‰")
                output.append("ğŸ’¡ æ•°æ®å¤„ç†å»ºè®®: å»ºè®®é‡æ–°è¯„ä¼°æ•°æ®é‡‡é›†æ–¹æ³•ï¼Œå¢åŠ å…³é”®æ•°æ®ç‚¹æµ‹é‡æ¬¡æ•°")
            elif rel_uncertainty < 30:
                output.append("âŒ ä¸ç¡®å®šåº¦ç­‰çº§: ä¸­é«˜")
                output.append("ğŸ’¡ æ•°æ®å¤„ç†å»ºè®®: å»ºè®®æ”¹è¿›å®éªŒæ–¹æ³•ï¼Œå¢åŠ æ ·æœ¬é‡ï¼Œé‡æ–°é‡‡é›†æ•°æ®")
            else:
                output.append("âŒ ä¸ç¡®å®šåº¦ç­‰çº§: é«˜")
                output.append("ğŸ’¡ æ•°æ®å¤„ç†å»ºè®®: å¿…é¡»é‡æ–°è®¾è®¡å®éªŒï¼Œæ”¹è¿›æµ‹é‡æ–¹æ³•ï¼Œæ”¶é›†æ–°æ•°æ®")
    
    # ç½®ä¿¡åº¦æŒ‡æ ‡ - ä¼˜åŒ–ç‰ˆï¼Œæ›´ç®€æ´æ˜“è¯»
    if 'confidence_measures' in quality and quality['confidence_measures']:
        confidence = quality['confidence_measures']
        output.append("\nğŸ” æ¨¡å‹ç½®ä¿¡åº¦æŒ‡æ ‡ ğŸ”")
        output.append("=" * 45)
        if 'adjusted_r_squared' in confidence:
            adj_r2 = confidence['adjusted_r_squared']
            output.append(f"è°ƒæ•´åçš„å†³å®šç³»æ•°(RÂ²): {adj_r2:.4f}")
            
            # è°ƒæ•´RÂ²è¯„ä»· - æ›´ç®€æ´çš„è¯„ä»·
            if adj_r2 >= 0.95:
                output.append(f"âœ… æ¨¡å‹è§£é‡ŠåŠ›: æé«˜")
                output.append(f"ğŸ’¡ ç½®ä¿¡åº¦è¯„ä¼°: æ¨¡å‹éå¸¸å¯é ï¼Œå¯ç”¨äºé«˜ç²¾åº¦é¢„æµ‹")
            elif adj_r2 >= 0.9:
                output.append(f"âœ… æ¨¡å‹è§£é‡ŠåŠ›: å¾ˆé«˜")
                output.append(f"ğŸ’¡ ç½®ä¿¡åº¦è¯„ä¼°: æ¨¡å‹å¯é ï¼Œé€‚åˆå¤§å¤šæ•°åº”ç”¨åœºæ™¯")
            elif adj_r2 >= 0.8:
                output.append(f"âš ï¸ æ¨¡å‹è§£é‡ŠåŠ›: é«˜")
                output.append(f"ğŸ’¡ ç½®ä¿¡åº¦è¯„ä¼°: æ¨¡å‹è¾ƒå¯é ï¼Œå¯ç”¨äºå¸¸è§„åˆ†æ")
            elif adj_r2 >= 0.7:
                output.append(f"âš ï¸ æ¨¡å‹è§£é‡ŠåŠ›: ä¸­ç­‰")
                output.append(f"ğŸ’¡ ç½®ä¿¡åº¦è¯„ä¼°: æ¨¡å‹åŸºæœ¬å¯é ï¼Œç»“æœä»…ä¾›å‚è€ƒ")
            elif adj_r2 >= 0.5:
                output.append(f"âŒ æ¨¡å‹è§£é‡ŠåŠ›: ä¸€èˆ¬")
                output.append(f"ğŸ’¡ ç½®ä¿¡åº¦è¯„ä¼°: æ¨¡å‹è§£é‡ŠåŠ›æœ‰é™ï¼Œéœ€è°¨æ…ä½¿ç”¨")
                output.append(f"ğŸ’¡ æ”¹è¿›å»ºè®®: è€ƒè™‘å…¶ä»–ç±»å‹çš„æ¨¡å‹æˆ–å¢åŠ æ•°æ®é‡")
            else:
                output.append(f"âŒ æ¨¡å‹è§£é‡ŠåŠ›: ä½")
                output.append(f"ğŸ’¡ ç½®ä¿¡åº¦è¯„ä¼°: æ¨¡å‹è§£é‡ŠåŠ›ä¸è¶³ï¼Œä¸å»ºè®®ç”¨äºé¢„æµ‹")
                output.append(f"ğŸ’¡ æ”¹è¿›å»ºè®®: å¿…é¡»é‡æ–°é€‰æ‹©æ¨¡å‹æˆ–è€ƒè™‘æ•°æ®é¢„å¤„ç†æ–¹æ³•")
    
    # ä¼˜åŒ–ç‰ˆå®ç”¨å»ºè®® - æ›´å…·é’ˆå¯¹æ€§å’Œå¯æ“ä½œæ€§
    output.append("\nğŸ’¡ æ•°æ®å¤„ç†ä¼˜åŒ–å»ºè®® ğŸ’¡")
    output.append("=" * 45)
    
    # åˆå§‹åŒ–å»ºè®®åˆ—è¡¨
    recommendations = []
    n_points = len(results['filtered_data'][0])
    outlier_percent = len(results['filtered_indices']) / len(results['original_data'][0]) * 100 if len(results['original_data'][0]) > 0 else 0
    
    # æ ·æœ¬é‡å»ºè®®
    if n_points < 10:
        recommendations.append(f"ğŸ“ˆ å¢åŠ æ ·æœ¬é‡è‡³å°‘è‡³10ä¸ªæ•°æ®ç‚¹ï¼Œå½“å‰æ ·æœ¬é‡({n_points})ä¸è¶³ä»¥æ”¯æ’‘å¯é çš„ç»Ÿè®¡åˆ†æ")
    elif n_points < 20:
        recommendations.append(f"ğŸ“ˆ è€ƒè™‘å¢åŠ æ ·æœ¬é‡è‡³20ä¸ªä»¥ä¸Šï¼Œä»¥æé«˜æ¨¡å‹çš„ç¨³å®šæ€§å’Œæ³›åŒ–èƒ½åŠ›")
    
    # å¼‚å¸¸ç‚¹å»ºè®®
    if outlier_percent > 20:
        recommendations.append(f"ğŸ” å¼‚å¸¸ç‚¹æ¯”ä¾‹è¿‡é«˜({outlier_percent:.1f}%)ï¼Œå»ºè®®æ£€æŸ¥å®éªŒæ¡ä»¶ï¼Œé‡æ–°é‡‡é›†æ•°æ®æˆ–ä½¿ç”¨ç¨³å¥ä¼°è®¡æ–¹æ³•")
    elif outlier_percent > 10:
        recommendations.append(f"ğŸ” å­˜åœ¨è¾ƒå¤šå¼‚å¸¸ç‚¹({outlier_percent:.1f}%)ï¼Œå»ºè®®éªŒè¯è¿™äº›æ•°æ®ç‚¹çš„å¯é æ€§")
    
    # æ¨¡å‹æ‹Ÿåˆå»ºè®®
    if results['best_poly_fit']:
        poly = results['best_poly_fit']
        if poly['degree'] > 4:
            recommendations.append(f"ğŸ§® å½“å‰å¤šé¡¹å¼é˜¶æ•°({poly['degree']})è¾ƒé«˜ï¼Œå¯èƒ½å­˜åœ¨è¿‡æ‹Ÿåˆé£é™©ï¼Œå»ºè®®å°è¯•é˜¶æ•°â‰¤3çš„æ¨¡å‹")
        elif poly['r_squared'] < 0.7 and results['filtered_stats'] and results['filtered_stats'].get('correlation', 0) > 0.5:
            recommendations.append("ğŸ§® è™½ç„¶æ•°æ®ç›¸å…³æ€§è¾ƒå¥½ï¼Œä½†å¤šé¡¹å¼æ‹Ÿåˆæ•ˆæœä¸€èˆ¬ï¼Œå»ºè®®å°è¯•éçº¿æ€§æ¨¡å‹")
    
    # æ®‹å·®åˆ†æå»ºè®®
    if 'residual_analysis' in quality and quality['residual_analysis']:
        residual_analysis = quality['residual_analysis']
        if abs(residual_analysis['residual_mean']) > 1e-6:
            recommendations.append("ğŸ“Š æ®‹å·®å­˜åœ¨ç³»ç»Ÿæ€§åå·®ï¼Œå»ºè®®æ£€æŸ¥æ•°æ®é‡‡é›†è¿‡ç¨‹ä¸­çš„ç³»ç»Ÿè¯¯å·®")
        if abs(residual_analysis['residual_skewness']) > 1.0:
            recommendations.append("ğŸ“Š æ®‹å·®åˆ†å¸ƒæ˜æ˜¾åæ–œï¼Œå»ºè®®è€ƒè™‘æ•°æ®å˜æ¢æˆ–å…¶ä»–æ¨¡å‹ç±»å‹")
    
    # ç›¸å…³æ€§å»ºè®®
    if results['filtered_stats'] and results['filtered_stats'].get('correlation', 0) and abs(results['filtered_stats']['correlation']) < 0.3:
        recommendations.append("ğŸ”— å˜é‡ç›¸å…³æ€§å¾ˆå¼±ï¼Œä¼ ç»Ÿæ‹Ÿåˆæ–¹æ³•å¯èƒ½ä¸é€‚ç”¨ï¼Œå»ºè®®é‡æ–°è€ƒè™‘æ•°æ®æ¨¡å‹æˆ–å®éªŒè®¾è®¡")
    
    # ä¸ç¡®å®šæ€§å»ºè®®
    if 'uncertainty_estimates' in quality and quality['uncertainty_estimates'] and 'relative_uncertainty' in quality['uncertainty_estimates']:
        if quality['uncertainty_estimates']['relative_uncertainty'] > 20:
            recommendations.append("âš ï¸ æ•°æ®ä¸ç¡®å®šæ€§è¾ƒé«˜ï¼Œå»ºè®®æ”¹è¿›æµ‹é‡ç²¾åº¦æˆ–å¢åŠ é‡å¤æµ‹é‡æ¬¡æ•°")
    
    # æ·»åŠ è´¨é‡è¯„ä¼°ä¸­çš„å»ºè®®
    if quality['recommendations']:
        for rec in quality['recommendations']:
            if rec not in recommendations:  # é¿å…é‡å¤
                recommendations.append(rec)
    
    # å¦‚æœæ²¡æœ‰å…·ä½“å»ºè®®ï¼Œæ·»åŠ é€šç”¨å»ºè®®
    if not recommendations:
        recommendations.append("âœ… å½“å‰æ•°æ®å’Œæ¨¡å‹æ‹Ÿåˆæƒ…å†µè‰¯å¥½ï¼Œæ— éœ€ç‰¹æ®Šè°ƒæ•´")
    
    # æ·»åŠ æœ€ç»ˆå»ºè®®åˆ—è¡¨ - é™åˆ¶æ•°é‡ï¼Œé¿å…ä¿¡æ¯è¿‡è½½
    max_recommendations = 5  # é™åˆ¶æœ€å¤šæ˜¾ç¤º5æ¡å»ºè®®
    for i, rec in enumerate(recommendations[:max_recommendations], 1):
        output.append(f"{i}. {rec}")
    
    # å¦‚æœæœ‰æ›´å¤šå»ºè®®ï¼Œæç¤ºç”¨æˆ·
    if len(recommendations) > max_recommendations:
        output.append(f"... è¿˜æœ‰ {len(recommendations) - max_recommendations} æ¡è¯¦ç»†å»ºè®®ï¼Œè¯·å‚è€ƒå®Œæ•´åˆ†æ")
    
    # æ€»ç»“æ€§è¯„ä»· - æ›´åŠ çªå‡ºå’Œç®€æ´
    output.append("\nğŸ† åˆ†ææ€»ç»“ ğŸ†")
    output.append("=" * 45)
    
    # è®¡ç®—ç»¼åˆè¯„åˆ†ï¼ˆ0-100åˆ†ï¼‰- ä¼˜åŒ–æƒé‡
    # æ‹Ÿåˆä¼˜åº¦å¾—åˆ† (0-35åˆ†)
    goodness_score = 0
    if quality['goodness_of_fit'] == 'ä¼˜ç§€':
        goodness_score = 35
    elif quality['goodness_of_fit'] == 'è‰¯å¥½':
        goodness_score = 28
    elif quality['goodness_of_fit'] == 'ä¸€èˆ¬':
        goodness_score = 20
    elif quality['goodness_of_fit'] == 'è¾ƒå·®':
        goodness_score = 10
    else:  # å¾ˆå·®
        goodness_score = 5
    
    # æ•°æ®ä»£è¡¨æ€§å¾—åˆ† (0-30åˆ†)
    representativeness_score = 0
    if quality['data_representativeness'] == 'éå¸¸å¥½':
        representativeness_score = 30
    elif quality['data_representativeness'] == 'è‰¯å¥½':
        representativeness_score = 25
    elif quality['data_representativeness'] == 'ä¸€èˆ¬':
        representativeness_score = 15
    else:  # è¾ƒå·®
        representativeness_score = 10
    
    # æ•°æ®è´¨é‡å¾—åˆ† (0-35åˆ†) - å¢åŠ å¯¹ç»Ÿè®¡ç‰¹æ€§çš„è€ƒè™‘
    data_quality_score = 35
    
    # å¼‚å¸¸ç‚¹æ‰£åˆ†
    if len(results['filtered_indices']) > 0:
        outlier_percent = len(results['filtered_indices']) / len(results['original_data'][0]) * 100
        if outlier_percent > 20:
            data_quality_score -= 15
        elif outlier_percent > 10:
            data_quality_score -= 10
        elif outlier_percent > 5:
            data_quality_score -= 5
    
    # æ ·æœ¬é‡æ‰£åˆ†
    n_points = len(results['filtered_data'][0])
    if n_points < 3:
        data_quality_score -= 15
    elif n_points < 5:
        data_quality_score -= 10
    elif n_points < 10:
        data_quality_score -= 5
    
    # ç›¸å…³æ€§åŠ åˆ†ï¼ˆå¦‚æœç›¸å…³ç³»æ•°è‰¯å¥½ï¼‰
    if results['filtered_stats'] and 'correlation' in results['filtered_stats']:
        corr = results['filtered_stats']['correlation']
        if abs(corr) >= 0.9:
            data_quality_score += 5
        elif abs(corr) >= 0.7:
            data_quality_score += 3
    
    # ç¡®ä¿åˆ†æ•°ä¸å°äº0
    data_quality_score = max(0, data_quality_score)
    
    # è®¡ç®—æ€»åˆ†
    total_score = goodness_score + representativeness_score + data_quality_score
    
    # æ€»ç»“è¯„çº§
    if total_score >= 90:
        grade = "ä¼˜ç§€"
        icon = "ğŸ†"
    elif total_score >= 80:
        grade = "è‰¯å¥½"
        icon = "ğŸ‘"
    elif total_score >= 70:
        grade = "ä¸€èˆ¬"
        icon = "ğŸ’¡"
    elif total_score >= 60:
        grade = "åŠæ ¼"
        icon = "âš ï¸"
    else:
        grade = "ä¸åŠæ ¼"
        icon = "âŒ"
    
    # æ·»åŠ æ€»ç»“è¯„ä»·
    output.append(f"ç»¼åˆè¯„åˆ†: {total_score}/100 {icon}")
    output.append(f"è¯„çº§: {grade}")
    
    # è¯¦ç»†æ€»ç»“ - æ›´ç®€æ´æœ‰åŠ›çš„æŒ‡å¯¼
    if grade == "ä¼˜ç§€":
        output.append("âœ… æ€»ä½“è¯„ä»·: æ•°æ®è´¨é‡ä¼˜ç§€ï¼Œæ‹Ÿåˆæ•ˆæœæä½³ï¼Œç»“æœå¯é æ€§é«˜ã€‚")
        output.append("ğŸ’¡ æ­¤æ•°æ®åˆ†æç»“æœå¯ç”¨äºå‘è¡¨ç ”ç©¶è®ºæ–‡æˆ–é‡è¦å†³ç­–ã€‚")
    elif grade == "è‰¯å¥½":
        output.append("âœ… æ€»ä½“è¯„ä»·: æ•°æ®è´¨é‡è‰¯å¥½ï¼Œæ‹Ÿåˆç»“æœå¯é ï¼Œå¯ç”¨äºå¸¸è§„åˆ†æå’Œå†³ç­–ã€‚")
        output.append("ğŸ’¡ å»ºè®®å®šæœŸéªŒè¯ç»“æœä»¥ç¡®ä¿ç¨³å®šæ€§ï¼Œå¹¶è€ƒè™‘ä¸Šè¿°ä¼˜åŒ–å»ºè®®è¿›ä¸€æ­¥æå‡è´¨é‡ã€‚")
    elif grade == "ä¸€èˆ¬":
        output.append("âš ï¸ æ€»ä½“è¯„ä»·: æ•°æ®è´¨é‡ä¸€èˆ¬ï¼Œæ‹Ÿåˆç»“æœæœ‰ä¸€å®šå‚è€ƒä»·å€¼ï¼Œä½†å­˜åœ¨æ˜æ˜¾æ”¹è¿›ç©ºé—´ã€‚")
        output.append("ğŸ’¡ è¯·åŠ¡å¿…æŒ‰ç…§ä¸Šè¿°å»ºè®®è¿›è¡Œä¼˜åŒ–ï¼Œç‰¹åˆ«æ˜¯å¢åŠ æ ·æœ¬é‡å’Œå¤„ç†å¼‚å¸¸ç‚¹ã€‚")
    elif grade == "åŠæ ¼":
        output.append("âš ï¸ æ€»ä½“è¯„ä»·: æ•°æ®è´¨é‡å’Œæ‹Ÿåˆæ•ˆæœåŸºæœ¬åˆæ ¼ï¼Œä½†å¯é æ€§æœ‰é™ã€‚")
        output.append("ğŸ’¡ ä¸å»ºè®®å°†ç»“æœç”¨äºé‡è¦å†³ç­–ï¼Œå¿…é¡»å¢åŠ æ•°æ®é‡å¹¶æ”¹è¿›å®éªŒæ–¹æ³•ã€‚")
    else:  # ä¸åŠæ ¼
        output.append("âŒ æ€»ä½“è¯„ä»·: æ•°æ®è´¨é‡è¾ƒå·®ï¼Œæ‹Ÿåˆç»“æœå¯é æ€§ä½ï¼Œæ— æ³•ç”¨äºä¸“ä¸šåˆ†æã€‚")
        output.append("ğŸ’¡ å¼ºçƒˆå»ºè®®é‡æ–°è®¾è®¡å®éªŒï¼Œæ”¹è¿›æµ‹é‡æ–¹æ³•ï¼Œæ”¶é›†æ–°æ•°æ®ã€‚")
    
    # é™„åŠ å»ºè®® - æ›´æœ‰é’ˆå¯¹æ€§å’Œå¯æ“ä½œæ€§
    output.append("\nğŸ”® åç»­å·¥ä½œå»ºè®® ğŸ”®")
    
    # åŸºäºä¸åŒåˆ†æ•°æ®µç»™å‡ºä¸åŒçº§åˆ«çš„å»ºè®®
    if grade in ['ä¼˜ç§€', 'è‰¯å¥½']:
        output.append("1. ğŸ§ª è€ƒè™‘å¯¹æ¨¡å‹è¿›è¡Œäº¤å‰éªŒè¯ï¼Œè¿›ä¸€æ­¥éªŒè¯å…¶æ³›åŒ–èƒ½åŠ›")
        output.append("2. ğŸ“Š å°è¯•ä¸åŒçš„æ¨¡å‹ç±»å‹ï¼Œæ¯”è¾ƒæ•ˆæœå·®å¼‚")
        output.append("3. ğŸ” è¿›è¡Œæ•æ„Ÿæ€§åˆ†æï¼Œè¯„ä¼°å…³é”®å‚æ•°å˜åŒ–å¯¹ç»“æœçš„å½±å“")
    elif grade in ['ä¸€èˆ¬', 'åŠæ ¼']:
        output.append("1. ğŸ“ˆ å¿…é¡»å¢åŠ æ ·æœ¬é‡ï¼Œç¡®ä¿æ•°æ®è¦†ç›–æ‰€æœ‰å…³é”®åŒºåŸŸ")
        output.append("2. ğŸ”§ é‡æ–°è®¾è®¡å®éªŒæµç¨‹ï¼Œå‡å°‘æµ‹é‡è¯¯å·®")
        output.append("3. ğŸ§® å°è¯•ä¸åŒçš„æ•°æ®é¢„å¤„ç†æ–¹æ³•å’Œæ¨¡å‹ç±»å‹")
    else:  # ä¸åŠæ ¼
        output.append("1. âš ï¸ ç«‹å³åœæ­¢ä½¿ç”¨å½“å‰æ•°æ®è¿›è¡Œå†³ç­–ï¼Œé‡æ–°è§„åˆ’å®éªŒ")
        output.append("2. ğŸ“š å’¨è¯¢ç»Ÿè®¡å­¦ä¸“å®¶ï¼Œè®¾è®¡æ›´åˆç†çš„å®éªŒæ–¹æ¡ˆ")
        output.append("3. ğŸ”§ æ”¹è¿›æµ‹é‡è®¾å¤‡å’Œæ–¹æ³•ï¼Œæé«˜æ•°æ®è´¨é‡")
    
    # é€šç”¨å»ºè®® - æ›´åŠ ç®€æ´å®ç”¨
    output.append("\nğŸ“‹ æ•°æ®å¤„ç†æœ€ä½³å®è·µ:")
    output.append("âœ… å§‹ç»ˆä¿æŒåŸå§‹æ•°æ®è®°å½•ï¼Œé¿å…æ•°æ®ä¸¢å¤±")
    output.append("âœ… è®°å½•å®éªŒæ¡ä»¶å’Œç¯å¢ƒå‚æ•°ï¼Œä¾¿äºåç»­åˆ†æ")
    output.append("âœ… å¯¹å¼‚å¸¸æ•°æ®è¿›è¡ŒéªŒè¯è€Œéç›´æ¥åˆ é™¤")
    output.append("âœ… ç»“åˆä¸“ä¸šçŸ¥è¯†è§£é‡Šç»Ÿè®¡ç»“æœï¼Œé¿å…æœºæ¢°è§£è¯»")
    
    return "\n".join(output)

def generate_experiment_plot_data(results: Dict[str, Any]) -> Dict[str, Any]:
    """ç”Ÿæˆç”¨äºç»˜å›¾çš„æ•°æ®"""
    plot_data = {
        'original_x': results['original_data'][0].tolist(),
        'original_y': results['original_data'][1].tolist(),
        'filtered_x': results['filtered_data'][0].tolist(),
        'filtered_y': results['filtered_data'][1].tolist(),
        'filtered_indices': results['filtered_indices']
    }
    
    # æ·»åŠ å¤šé¡¹å¼æ‹Ÿåˆæ›²çº¿
    if results['best_poly_fit']:
        x_min, x_max = min(results['filtered_data'][0]), max(results['filtered_data'][0])
        x_curve = np.linspace(x_min * 0.9, x_max * 1.1, 1000)
        poly_func = np.poly1d(results['best_poly_fit']['coeffs'])
        y_curve = poly_func(x_curve)
        plot_data['poly_fit_x'] = x_curve.tolist()
        plot_data['poly_fit_y'] = y_curve.tolist()
    
    # æ·»åŠ å¹³æ»‘æ›²çº¿ï¼ˆå¦‚æœæœ‰ï¼‰
    if results['smooth_curve']:
        plot_data['smooth_curve_x'] = results['smooth_curve'][0].tolist()
        plot_data['smooth_curve_y'] = results['smooth_curve'][1].tolist()
    
    return plot_data